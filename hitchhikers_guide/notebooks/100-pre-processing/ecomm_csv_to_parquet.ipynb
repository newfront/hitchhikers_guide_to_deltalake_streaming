{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2b56ce6-25e2-4c1d-a28e-8dcdbfb8e0f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run first. then have fun.\n",
    "from pyspark.sql.functions import col, to_date, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, FloatType\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20125793-7bf3-41b4-9595-07f2dfa4b328",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate the Schema (StructType) of the CSV Data (ecomm_behavior_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "561bfd71-47e5-4d11-86a0-43bf806b62f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, FloatType\n",
    "schema = (StructType([\n",
    "    StructField(\"event_time\", StringType(), False),\n",
    "    StructField(\"event_type\", StringType(), False),\n",
    "    StructField(\"product_id\", IntegerType(), False),\n",
    "    StructField(\"category_id\", LongType(), False),\n",
    "    StructField(\"category_code\", StringType(), False),\n",
    "    StructField(\"brand\", StringType(), False),\n",
    "    StructField(\"price\", FloatType(), False),\n",
    "    StructField(\"user_id\", IntegerType(), False),\n",
    "    StructField(\"user_session\", StringType(), False),\n",
    "]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d08b64c4-2143-4a23-b65d-7aec7df072d2",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "> Note: The github repo contains the `-sm.csv` data sampled from [Kaggle: Ecommerce Behavior Data Multi Category Store](https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store). To follow along with the complete dataset, just download it and drop the `2019-Oct.csv, 2019-Nov.csv` files into the `datasets` directory in the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32e09baa-3edc-4a84-899f-e2b5b46e2c76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_dir = '/opt/spark/work-dir/hitchhikers_guide/datasets/ecomm_behavior_data'\n",
    "# note: if you download the full dataset from https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store,\n",
    "# just use the following and comment out the `-sm.csv` datasets.\n",
    "#datasets = ['2019-Oct.csv','2019-Nov.csv']\n",
    "datasets = ['2019-Oct-sm.csv','2019-Nov-sm.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76790fab-05d2-41b0-856c-f289205be2ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read and process initial dataset\n",
    "\n",
    "ecomm_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .schema(schema)\n",
    "    .load(f\"{dataset_dir}/{datasets[1]}\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f2828a-8418-4277-84f1-7f272e9b3fd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ecomm_df.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd466561-36fc-4d4a-8448-3da5c0bdb326",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ecomm_df.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22874dd2-17a5-4a1d-9a67-8467878d3749",
   "metadata": {},
   "source": [
    "## Convert from CSV to Partitioned Parquet\n",
    "While there is a simplicity to using CSV. It can be a problematic format to work with. Luckily, the ecomm dataset has already been preprocessed (cleaned).\n",
    "\n",
    "**What we'll achieve**\n",
    "1. We will do some minor post-processing, converting the `event_time` from a StringType to a DateTimeType. To do that we will be using the `to_timestamp` function. You'll notice that we need to `format` the timestamp conversion given we are parsing a string and need to reflect the format. `2019-10-01 00:00:00 UTC` is referenced using `yyyy-MM-dd HH:mm:ss z`.\n",
    "2. Given the size of the data (~9GB for Nov, ~4GB for Oct) it also makes sense to partition by day (to speed up local processing). On that note, we also need to create a new column called `event_date` in order to store the partition information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18310bba-5328-41b4-a15f-6e4766944bd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, to_timestamp\n",
    "spark.conf.set(\"spark.sql.parquet.compression.codec\", \"zstd\")\n",
    "\n",
    "sink_dir = 'sm' if datasets[1].endswith('-sm.csv') else 'lg'\n",
    "\n",
    "(ecomm_df\n",
    "   .withColumn(\"event_time\", to_timestamp(col(\"event_time\"), \"yyyy-MM-dd HH:mm:ss z\"))\n",
    "   .withColumn(\"event_date\", to_date(col(\"event_time\")))\n",
    "   .write\n",
    "   .format(\"parquet\")\n",
    "   .partitionBy(\"event_date\")\n",
    "   .mode(\"append\")\n",
    "   .save(f\"{dataset_dir}/parquet/{sink_dir}\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "caf57ea0-b232-4013-bfb1-8f90841fece0",
   "metadata": {},
   "source": [
    "## Read Back our Parquet by Specific Date\n",
    "> Note: Now that we have our schema (StructType) set, this will be encoded into the Parquet data. This simplifies reading back from our new parquet location (as long as we don't screw up or modify the schema since parquet doesn't have any notion of Schema Enforcement. This is a plus of working with Delta Lake, which we'll see in the rest of the Hitchhiker's Guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546e9fea-7619-4008-b71f-0ad7a14779c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Run a gutcheck on one of the days. See how things work. Probably pretty fast.\n",
    "# if you've imported the `lg` data - switch \n",
    "\n",
    "source_dir = 'sm' if datasets[1].endswith('-sm.csv') else 'lg'\n",
    "(spark.read\n",
    " .format(\"parquet\")\n",
    " .load(f\"{dataset_dir}/parquet/{source_dir}/\")\n",
    " .where(col(\"event_date\").eqNullSafe(\"2019-10-01\"))\n",
    " .show(10)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f67e89ce-46e4-453d-9233-1dcb10e189d4",
   "metadata": {},
   "source": [
    "## Where to go Next?\n",
    "> This notebook only exists to help read, post-process, and write partitioned data from the ecomm dataset. We will be using the `parquet` data for the actual hitchhiker's guide. \n",
    "\n",
    "Now that we've learned how to read and write our csv data, it is time to actually tackle problems using Delta Lake. If you are new to using Delta Lake, then it is easiest to head over to [First Steps](../101-first-steps/README.md) to learn how to create and modify Delta Lake tables (with the intention of going from zero-to-hero)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
