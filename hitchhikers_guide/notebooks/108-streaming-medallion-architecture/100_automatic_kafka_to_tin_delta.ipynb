{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "707e24cf-53be-437a-b242-14386e7c948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DateType\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.streaming import StreamingQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acea944-df23-4042-94a8-5d4e67044891",
   "metadata": {},
   "source": [
    "## Setup the Application\n",
    "> This includes the following:\n",
    "* app name\n",
    "* app version\n",
    "* kafka topic: `coffeeco.v1.orders`\n",
    "* checkpoint dir, and path (for the streaming application state)\n",
    "* the sink location: Managed Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "377ce8e4-510a-4593-be89-b07cfa3d6283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint_path=/opt/spark/work-dir/hitchhikers_guide/applications/ringmaster_kafka_tin/v0.0.1/_checkpoints\n"
     ]
    }
   ],
   "source": [
    "app_name = \"ringmaster_kafka_tin\"\n",
    "app_version = \"v0.0.1\"\n",
    "kafka_topic = \"coffeeco.v1.orders\"\n",
    "kafka_brokers = \"kafka-rp:29092\"\n",
    "checkpoint_dir = \"/opt/spark/work-dir/hitchhikers_guide/applications\"\n",
    "checkpoint_path = f\"{checkpoint_dir}/{app_name}/{app_version}/_checkpoints\"\n",
    "print(f\"checkpoint_path={checkpoint_path}\")\n",
    "\n",
    "kafka_to_delta_table = \"default.coffeeco_v1_orders_tin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aac3b1e-6809-4598-b2b8-8df9549abdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_brokers)\n",
    "    .option(\"subscribe\", kafka_topic)\n",
    "    .option(\"failOnDataLoss\", \"true\")\n",
    "    .option(\"mode\", \"FAIL_FAST\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"fetchOffset.retryIntervalMs\", \"10\")\n",
    "    .option(\"groupIdPrefix\", \"ringmaster_tin\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae976ddf-b394-43a7-b6f5-2836d7832733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d676157b-3247-476e-b62e-2535e67b7ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_df = kafka_df.select(\"key\", \"value\", \"timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7380fe8-96b5-4d05-ad4d-11db818f4cc0",
   "metadata": {},
   "source": [
    "## Create the Managed Delta Table\n",
    "> This will allow us to automatically take the streaming protobuf data and drop it into a \"tin\" landing zone Delta table.\n",
    "> By taking the streaming data off of Kafka, we erase one of the trickier aspects of working with short-TTLs from Kafka (typically between 24-48 hours for large topics). If things need to be replayed, we can now reference our Delta Table rather than rushing to recover while data may already be lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4be5b16a-dac5-4a7d-b184-3c9df6e95c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt: DeltaTable = (\n",
    "    DeltaTable.createIfNotExists(spark)\n",
    "    .tableName(kafka_to_delta_table)\n",
    "    .addColumns(projection_df.schema)\n",
    "    .addColumn(\"date\", DateType(), generatedAlwaysAs=\"CAST(timestamp AS DATE)\")\n",
    "    .comment(\"kafka-rp:coffeeco.v1.orders:tin\")\n",
    "    .clusterBy(\"date\")\n",
    "    .property(\"description\", \"This table provides a home for the processed Kafka records without immediate fear of TTLs\")\\\n",
    "    .property(\"delta.logRetentionDuration\", \"interval 30 days\")\n",
    "    .property(\"delta.deletedFileRetentionDuration\", \"interval 7 days\")\n",
    "    .property(\"delta.dataSkippingNumIndexedCols\", \"4\")\n",
    "    .property(\"delta.checkpoint.writeStatsAsStruct\", \"true\")\n",
    "    .property(\"delta.checkpoint.writeStatsAsJson\", \"false\")\n",
    "    .property(\"delta.checkpointPolicy\", \"v2\")\n",
    "    .property(\"delta.enableDeletionVectors\", \"true\")\n",
    "    .execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb42e4b-9326-408b-90e0-42953153bdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.conf.get(\"spark.sql.warehouse.dir\")\n",
    "# // file:/opt/spark/work-dir/hitchhikers_guide/warehouse\n",
    "#spark.catalog.currentCatalog()\n",
    "#spark.catalog.currentDatabase()\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed1fb36-d64f-43f1-b319-b8293b1895bb",
   "metadata": {},
   "source": [
    "## Now that we have the Delta table. It is time to read from Kafka and populate the Tin Layer in the Streaming Medallion Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4c1e13-800c-4386-aede-b78304b464c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query: StreamingQuery = (\n",
    "    projection_df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"overwriteSchema\", \"false\")\n",
    "    .option(\"mergeSchema\", \"false\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(tableName=kafka_to_delta_table)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d290b6f1-188a-4eea-8805-667bedcf16a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streaming_query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5419e174-095c-40ed-809d-dca816372bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6664bffa-3bbb-4a41-87e2-bc765680e898",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_tin: DeltaTable = DeltaTable.forName(spark, kafka_to_delta_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ff51e91-e67c-4650-9970-c24683ae0257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_tin.toDF().printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2329a86d-26a7-4115-bf23-875afe39f92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+\n",
      "|                 key|               value|           timestamp|      date|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "|[53 63 6F 74 74 2...|[0A 0B 08 91 FD 8...|2024-06-07 21:41:...|2024-06-07|\n",
      "|[53 63 6F 74 74 2...|[0A 0C 08 E1 FD 8...|2024-06-07 21:42:...|2024-06-07|\n",
      "|[53 63 6F 74 74 2...|[0A 0C 08 FC FD 8...|2024-06-07 21:43:...|2024-06-07|\n",
      "|[53 63 6F 74 74 2...|[0A 0C 08 C8 84 8...|2024-06-07 21:57:...|2024-06-07|\n",
      "|[53 63 6F 74 74 2...|[0A 0C 08 93 86 8...|2024-06-07 22:00:...|2024-06-07|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_tin.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93992bb4-ac07-467b-b2d3-4a857042c183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
