{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca64dfa0-05a2-4b2b-8bc7-352e21a3584e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting spark-expectations\n",
      "  Downloading spark_expectations-2.1.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.10/dist-packages (from spark-expectations) (2.32.3)\n",
      "Collecting pluggy>=1\n",
      "  Downloading pluggy-1.5.0-py3-none-any.whl (20 kB)\n",
      "Collecting pyspark<3.5,>=3.0.0\n",
      "  Downloading pyspark-3.4.3.tar.gz (311.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.4/311.4 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark<3.5,>=3.0.0->spark-expectations) (0.10.9.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.28.1->spark-expectations) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.28.1->spark-expectations) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.28.1->spark-expectations) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.28.1->spark-expectations) (2024.6.2)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.4.3-py2.py3-none-any.whl size=311885516 sha256=f08606276b40198def92711a9fade25e08bdb42ec4a6c56ddf88c167debe964b\n",
      "  Stored in directory: /home/NBuser/.cache/pip/wheels/ea/a9/64/3713eb2c5048c18bae2778b013e5fc74203f5c22d4640fb776\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pyspark, pluggy, spark-expectations\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "delta-spark 3.2.0 requires pyspark<3.6.0,>=3.5.0, but you have pyspark 3.4.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pluggy-1.5.0 pyspark-3.4.3 spark-expectations-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install spark-expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21ba25cc-09b0-4157-97eb-7fdddccc836f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/13 00:10:38 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`default`.`coffeeco_tin_dq_rules` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop if rules table exists\n",
    "spark.sql(\"drop table if exists default.coffeeco_tin_dq_rules\")\n",
    "\n",
    "# create rules table \n",
    "spark.sql(\"\"\"create table if not exists default.coffeeco_tin_dq_rules (\n",
    "    product_id STRING,  \n",
    "    table_name STRING,  \n",
    "    rule_type STRING,  \n",
    "    rule STRING,  \n",
    "    column_name STRING,  \n",
    "    expectation STRING,  \n",
    "    action_if_failed STRING,  \n",
    "    tag STRING,  \n",
    "    description STRING,  \n",
    "    enable_for_source_dq_validation BOOLEAN,  \n",
    "    enable_for_target_dq_validation BOOLEAN,  \n",
    "    is_active BOOLEAN,  \n",
    "    enable_error_drop_alert BOOLEAN,  \n",
    "    error_drop_threshold INT,  \n",
    "    query_dq_delimiter STRING,  \n",
    "    enable_querydq_custom_output BOOLEAN\n",
    ")\n",
    "    USING delta\"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f69c56d9-2bd6-4411-b5f3-465557122947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- table_name: string (nullable = true)\n",
      " |-- rule_type: string (nullable = true)\n",
      " |-- rule: string (nullable = true)\n",
      " |-- column_name: string (nullable = true)\n",
      " |-- expectation: string (nullable = true)\n",
      " |-- action_if_failed: string (nullable = true)\n",
      " |-- tag: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- enable_for_source_dq_validation: boolean (nullable = true)\n",
      " |-- enable_for_target_dq_validation: boolean (nullable = true)\n",
      " |-- is_active: boolean (nullable = true)\n",
      " |-- enable_error_drop_alert: boolean (nullable = true)\n",
      " |-- error_drop_threshold: integer (nullable = true)\n",
      " |-- query_dq_delimiter: string (nullable = true)\n",
      " |-- enable_querydq_custom_output: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from default.coffeeco_tin_dq_rules\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4afbd7f9-4760-423d-b22b-beea0b85852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_yaml=\"\"\"\n",
    "rules:\n",
    "- action_if_failed: fail\n",
    "  column_name: total_nanos\n",
    "  description: total_nanos should not be null\n",
    "  enable_error_drop_alert: false\n",
    "  enable_for_source_dq_validation: false\n",
    "  enable_for_target_dq_validation: false\n",
    "  enable_querydq_custom_output: null\n",
    "  error_drop_threshold: 0\n",
    "  expectation: total_nanos is not null\n",
    "  is_active: true\n",
    "  product_id: coffeeco\n",
    "  query_dq_delimiter: null\n",
    "  rule: total_nanos_is_not_null\n",
    "  rule_type: row_dq\n",
    "  table_name: default.coffeeco_v1_orders_bronze\n",
    "  tag: accuracy\n",
    "- action_if_failed: drop\n",
    "  column_name: store_closed_permanently_on\n",
    "  description: store_closed_permanently_on should be greater than when the order is created, as closed stores cannot have orders\n",
    "  enable_error_drop_alert: false\n",
    "  enable_for_source_dq_validation: true\n",
    "  enable_for_target_dq_validation: true\n",
    "  enable_querydq_custom_output: null\n",
    "  error_drop_threshold: 0\n",
    "  expectation: store_closed_permanently_on > timestamp\n",
    "  is_active: true\n",
    "  product_id: coffeeco\n",
    "  query_dq_delimiter: null\n",
    "  rule: closed_store_shouldnt_have_orders\n",
    "  rule_type: row_dq\n",
    "  table_name: default.coffeeco_v1_orders_bronze\n",
    "  tag: validity\n",
    "- action_if_failed: ignore\n",
    "  column_name: total_units\n",
    "  description: sum of total units should be greater than 200\n",
    "  enable_error_drop_alert: false\n",
    "  enable_for_source_dq_validation: false\n",
    "  enable_for_target_dq_validation: true\n",
    "  enable_querydq_custom_output: null\n",
    "  error_drop_threshold: 0\n",
    "  expectation: sum(total_units) > 200\n",
    "  is_active: true\n",
    "  product_id: coffeeco\n",
    "  query_dq_delimiter: null\n",
    "  rule: sum_total_units_gt_200\n",
    "  rule_type: agg_dq\n",
    "  table_name: default.coffeeco_v1_orders_bronze\n",
    "  tag: validity\n",
    "- action_if_failed: ignore\n",
    "  column_name: order_created,store_id\n",
    "  description: detect duplicates\n",
    "  enable_error_drop_alert: false\n",
    "  enable_for_source_dq_validation: false\n",
    "  enable_for_target_dq_validation: true\n",
    "  enable_querydq_custom_output: True\n",
    "  error_drop_threshold: 0\n",
    "  expectation: (select count(*) from (select order_created, store_id from coffeeco_bronze_view t1 join default.coffeeco_v1_orders_bronze t2  on t1.order_created=t2.order_created, t1.store_id=t2.store_id)) = 0\n",
    "  is_active: true\n",
    "  product_id: coffeeco\n",
    "  query_dq_delimiter: null\n",
    "  rule: duplication_records_should_not_exist\n",
    "  rule_type: query_dq\n",
    "  table_name: default.coffeeco_v1_orders_bronze\n",
    "  tag: validity\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b744ca7d-929b-45db-921c-327f58f81a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "rules:\n",
      "- action_if_failed: fail\n",
      "  column_name: total_nanos\n",
      "  description: total_nanos should not be null\n",
      "  enable_error_drop_alert: false\n",
      "  enable_for_source_dq_validation: false\n",
      "  enable_for_target_dq_validation: false\n",
      "  enable_querydq_custom_output: null\n",
      "  error_drop_threshold: 0\n",
      "  expectation: total_nanos is not null\n",
      "  is_active: true\n",
      "  product_id: coffeeco\n",
      "  query_dq_delimiter: null\n",
      "  rule: total_nanos_is_not_null\n",
      "  rule_type: row_dq\n",
      "  table_name: default.coffeeco_v1_orders_bronze\n",
      "  tag: accuracy\n",
      "- action_if_failed: drop\n",
      "  column_name: store_closed_permanently_on\n",
      "  description: store_closed_permanently_on should be greater than when the order is created, as closed stores cannot have orders\n",
      "  enable_error_drop_alert: false\n",
      "  enable_for_source_dq_validation: true\n",
      "  enable_for_target_dq_validation: true\n",
      "  enable_querydq_custom_output: null\n",
      "  error_drop_threshold: 0\n",
      "  expectation: `store_closed_permanently_on` > `timestamp`\n",
      "  is_active: true\n",
      "  product_id: coffeeco\n",
      "  query_dq_delimiter: null\n",
      "  rule: closed_store_shouldnt_have_orders\n",
      "  rule_type: row_dq\n",
      "  table_name: default.coffeeco_v1_orders_bronze\n",
      "  tag: validity\n",
      "- action_if_failed: ignore\n",
      "  column_name: total_units\n",
      "  description: sum of total units should be greater than 200\n",
      "  enable_error_drop_alert: false\n",
      "  enable_for_source_dq_validation: false\n",
      "  enable_for_target_dq_validation: true\n",
      "  enable_querydq_custom_output: null\n",
      "  error_drop_threshold: 0\n",
      "  expectation: sum(total_units) > 200\n",
      "  is_active: true\n",
      "  product_id: coffeeco\n",
      "  query_dq_delimiter: null\n",
      "  rule: sum_total_units_gt_200\n",
      "  rule_type: agg_dq\n",
      "  table_name: default.coffeeco_v1_orders_bronze\n",
      "  tag: validity\n",
      "- action_if_failed: ignore\n",
      "  column_name: order_created,store_id\n",
      "  description: detect duplicates\n",
      "  enable_error_drop_alert: false\n",
      "  enable_for_source_dq_validation: false\n",
      "  enable_for_target_dq_validation: true\n",
      "  enable_querydq_custom_output: True\n",
      "  error_drop_threshold: 0\n",
      "  expectation: (select count(*) from (select order_created, store_id from coffeeco_bronze_view t1 join default.coffeeco_v1_orders_bronze t2  on t1.order_created=t2.order_created, t1.store_id=t2.store_id)) = 0\n",
      "  is_active: true\n",
      "  product_id: coffeeco\n",
      "  query_dq_delimiter: null\n",
      "  rule: duplication_records_should_not_exist\n",
      "  rule_type: query_dq\n",
      "  table_name: default.coffeeco_v1_orders_bronze\n",
      "  tag: validity\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rules_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21f1f5e0-7581-473c-9f32-70bc1cc76ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a8293b-1806-4df7-9e00-f50b3bd09120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Rules\n",
    "import yaml\n",
    "from pyspark.sql.types import StringType, BooleanType, IntegerType, StructType, StructField\n",
    "\n",
    "database = \"default\"\n",
    "rule_table_name = \"coffeeco_tin_dq_rules\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"product_id\", StringType(), nullable=False),\n",
    "    StructField(\"table_name\", StringType(), nullable=False),\n",
    "    StructField(\"rule_type\", StringType(), nullable=False),\n",
    "    StructField(\"rule\", StringType(), nullable=False),\n",
    "    StructField(\"column_name\", StringType(), nullable=False),\n",
    "    StructField(\"expectation\", StringType(), nullable=False),\n",
    "    StructField(\"action_if_failed\", StringType(), nullable=False),\n",
    "    StructField(\"tag\", StringType(), nullable=False),\n",
    "    StructField(\"description\", StringType(), nullable=False),\n",
    "    StructField(\"enable_for_source_dq_validation\", BooleanType(), nullable=False),\n",
    "    StructField(\"enable_for_target_dq_validation\", BooleanType(), nullable=False),\n",
    "    StructField(\"is_active\", BooleanType(), nullable=False),\n",
    "    StructField(\"enable_error_drop_alert\", BooleanType(), nullable=False),\n",
    "    StructField(\"error_drop_threshold\", IntegerType(), nullable=False),\n",
    "    StructField(\"query_dq_delimiter\", StringType(), nullable=True),\n",
    "    StructField(\"enable_querydq_custom_output\", BooleanType(), nullable=True)\n",
    "])\n",
    "\n",
    "rules_data = yaml.safe_load(rules_yaml)\n",
    "rules_df = spark.createDataFrame(rules_data[\"rules\"], schema)\n",
    "rules_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{database}.{rule_table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3862bb3c-fc09-4554-96f2-5500a8da2619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[product_id: string, table_name: string, rule_type: string, rule: string, column_name: string, expectation: string, action_if_failed: string, tag: string, description: string, enable_for_source_dq_validation: boolean, enable_for_target_dq_validation: boolean, is_active: boolean, enable_error_drop_alert: boolean, error_drop_threshold: int, query_dq_delimiter: string, enable_querydq_custom_output: boolean]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "dt = DeltaTable.forName(spark, \"default.coffeeco_tin_dq_rules\")\n",
    "dt.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0e7eb188-d7c2-4e04-8cd1-6ae17dd52270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Tin table as a streaming source\n",
    "coffee_orders_df = spark.readStream.format(\"delta\")\\\n",
    "    .option(\"withEventTimeOrder\", \"true\")\\\n",
    "    .table(\"default.coffeeco_v1_orders_tin\")\\\n",
    "    .withWatermark(\"timestamp\", \"10 seconds\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c0d4b6df-7ad9-4b51-86b1-8a771cc20b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denorm the data in Tin as needed\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def denorm(df):\n",
    "    denorm_df = coffee_orders_df.select(\n",
    "        \"date\",\n",
    "        \"timestamp\",\n",
    "        col(\"order.order_created\").alias(\"order_created\"),\n",
    "        col(\"order.purchased_at.store_id\").alias(\"store_id\"),\n",
    "        col(\"order.purchased_at.created\").alias(\"store_created\"),\n",
    "        col(\"order.purchased_at.opened_on\").alias(\"store_opened_on\"),\n",
    "        col(\"order.purchased_at.closed_permanently_on\").alias(\"store_closed_permanently_on\"),\n",
    "        col(\"order.purchased_at.status\").alias(\"store_status\"),\n",
    "        col(\"order.customer.name\").alias(\"customer_name\"),\n",
    "        col(\"order.customer.uuid\").alias(\"customer_uuid\"),\n",
    "        col(\"order.customer.first_seen\").alias(\"customer_first_seen\"),\n",
    "        col(\"order.customer.customer_type\").alias(\"customer_type\"),\n",
    "        col(\"order.customer.loyalty_member_id\").alias(\"loyalty_member_id\"),\n",
    "        col(\"order.items\").alias(\"items\"),\n",
    "        col(\"order.total.currency\").alias(\"total_currency\"),\n",
    "        col(\"order.total.units\").alias(\"total_units\"),\n",
    "        col(\"order.total.nanos\").alias(\"total_nanos\")\n",
    "    )\n",
    "    first_row = denorm_df.limit(1)\n",
    "    row_to_copy = row_to_copy.withColumn(\"store_closed_permanently_on\", lit(\"2024-01-01 00:00:00\"))\n",
    "    return denorm_df.union(row_to_copy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268752e8-4e92-4b56-8c83-0e74415841d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each batch run  Spark Expectations before writing into Bronze Layer\n",
    "from datetime import date\n",
    "from spark_expectations.core.expectations import (\n",
    "    SparkExpectations,\n",
    "    WrappedDataFrameWriter,\n",
    ")\n",
    "from spark_expectations.config.user_config import Constants as user_config\n",
    "\n",
    "def write_to_bronze(df, batch_id):\n",
    "    writer = WrappedDataFrameWriter().mode(\"append\").format(\"delta\")\n",
    "    se: SparkExpectations = SparkExpectations(\n",
    "        product_id=\"coffeeco\",\n",
    "        rules_df=spark.table(\"default.coffeeco_tin_dq_rules\"),\n",
    "        stats_table=\"default.coffeeco_dq_stats\",\n",
    "        stats_table_writer=writer,\n",
    "        target_and_error_table_writer=writer,\n",
    "        debugger=False,\n",
    "        # stats_streaming_options={user_config.se_enable_streaming: False},\n",
    "    )\n",
    "    \n",
    "    dic_job_info = {\n",
    "        \"job\": \"coffeeco_load_tin_bronze\",\n",
    "        \"Region\": \"NA\",\n",
    "        \"Snapshot\": date.today(),\n",
    "    }\n",
    "    \n",
    "    user_conf = {\n",
    "        user_config.se_notifications_enable_email: False,\n",
    "        user_config.se_notifications_email_smtp_host: \"mailhost.com\",\n",
    "        user_config.se_notifications_email_smtp_port: 25,\n",
    "        user_config.se_notifications_email_from: \"\",\n",
    "        user_config.se_notifications_email_to_other_mail_id: \"\",\n",
    "        user_config.se_notifications_email_subject: \"spark expectations - data quality - notifications\",\n",
    "        user_config.se_notifications_enable_slack: False,\n",
    "        user_config.se_notifications_slack_webhook_url: \"\",\n",
    "        user_config.se_notifications_on_start: True,\n",
    "        user_config.se_notifications_on_completion: True,\n",
    "        user_config.se_notifications_on_fail: True,\n",
    "        user_config.se_notifications_on_error_drop_exceeds_threshold_breach: True,\n",
    "        user_config.se_notifications_on_error_drop_threshold: 15,\n",
    "        user_config.se_enable_query_dq_detailed_result: True,\n",
    "        user_config.se_enable_agg_dq_detailed_result: True,\n",
    "        # user_config.querydq_output_custom_table_name: \"default.dq_stats_detailed_outputt\",\n",
    "        user_config.se_enable_error_table: True,\n",
    "        user_config.se_dq_rules_params: {\n",
    "            \"env\": \"dev\",\n",
    "            \"table\": \"coffeeco_v1_orders_bronze\",\n",
    "        },\n",
    "        user_config.se_job_metadata: str(dic_job_info),\n",
    "    }\n",
    "\n",
    "    @se.with_expectations(\n",
    "        target_table=\"default.coffeeco_v1_orders_bronze\",\n",
    "        write_to_table=True,\n",
    "        write_to_temp_table=False,\n",
    "        user_conf=user_conf,\n",
    "        target_table_view=\"coffeeco_bronze_view\"\n",
    "    )\n",
    "    def build_bronze(bronze_df):\n",
    "        return bronze_df\n",
    "\n",
    "    build_bronze(denorm(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986c735f-ed1d-4317-aff8-17a93b6a725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to bronze Layer\n",
    "query = coffee_orders_df.writeStream\\\n",
    "    .foreachBatch(write_to_bronze)\\\n",
    "    .queryName(\"WriteToBronzeWithSE\")\\\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\\\n",
    "    # .trigger(processingTime='5 seconds')\\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d743d86-6b19-4ab7-b350-467403405748",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db645b65-4290-4a91-a80e-075605cb134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Bronze Table\n",
    "coffee_v1_bronze_df = spark.sql(\"select * from default.coffeeco_v1_orders_bronze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea95580-1a8b-401c-a92e-f0fa839fd764",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_v1_bronze_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05cf79a-760c-4583-9173-e979d8c052fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the error table from bronze layer which has dropped records from SE\n",
    "coffee_v1_bronze_errors_df = spark.sql(\"select * from default.coffeeco_v1_orders_bronze_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c666bece-6492-4ed1-812c-752ac59c7b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_v1_bronze_errors_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da76a008-e226-41f3-ab79-758b727e57bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_v1_bronze_errors_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf39e2c-a752-426b-adae-f9514d72f3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the stats table\n",
    "stats_df = spark.sql(\"select * from default.coffeeco_dq_stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93fa12c-33ae-4138-8fe0-7f6683d00a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb29685c-eadd-43db-b509-feb764ee84d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a658c8-9043-478a-b798-83280082322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the stats detailed table\n",
    "stats_detailed_df = spark.sql(\"select * from default.coffeeco_dq_stats_detailed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e781460a-9dbe-410d-8305-abaa3f308d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_detailed_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061de6c5-5abb-4a42-a99a-1cb4c5d5a121",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_detailed_df.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
