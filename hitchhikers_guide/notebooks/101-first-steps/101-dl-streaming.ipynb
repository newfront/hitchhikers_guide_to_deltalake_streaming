{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a457036d-1d98-46e0-a64e-52faa61300e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run first. then have fun.\n",
    "from pyspark.sql.functions import col, current_timestamp, to_date, datediff\n",
    "# stats and agg functions\n",
    "from pyspark.sql.functions import count, session_window, window, sum, min, max, percentile_approx\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# keep the default compression codec as zstd\n",
    "spark.conf.set(\"spark.sql.parquet.compression.codec\", \"zstd\")\n",
    "\n",
    "# common dirs, paths\n",
    "dataset_dir = '/opt/spark/work-dir/hitchhikers_guide/datasets/ecomm_behavior_data'\n",
    "delta_path = f\"{dataset_dir}/delta\"\n",
    "\n",
    "# managed table information (from 100-streaming-first-steps)\n",
    "dl_table_name = \"ecomm_by_day\"\n",
    "dl_managed_table = f\"default.{dl_table_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c37f33a-778d-446f-b041-2820c1a257b2",
   "metadata": {},
   "source": [
    "# Intro to Delta Lake Streaming\n",
    "The following section will reuse the **Delta Lake** `default.ecomm_by_day` table created during [Streaming First Steps](./streaming-first-steps.ipynb).\n",
    "\n",
    "> note: run the following cell to check if you have the local table. You should see `[Table(name='ecomm_by_day', database='default', description=None, tableType='MANAGED', isTemporary=False)]` somewhere in the list (if you have more than one from the work in the Guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4a3f372-ea71-436e-9182-25d66c7b989f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://maven-central.storage-download.googleapis.com/maven2/ added as a remote repository with the name: repo-1\n",
      "Ivy Default Cache set to: /tmp\n",
      "The jars for the packages stored in: /tmp/jars\n",
      "org.apache.logging.log4j#log4j-api added as a dependency\n",
      "org.apache.derby#derby added as a dependency\n",
      "org.apache.hive#hive-metastore added as a dependency\n",
      "org.apache.hive#hive-exec added as a dependency\n",
      "org.apache.hive#hive-common added as a dependency\n",
      "org.apache.hive#hive-serde added as a dependency\n",
      "com.google.guava#guava added as a dependency\n",
      "org.apache.hadoop#hadoop-client-api added as a dependency\n",
      "org.apache.hadoop#hadoop-client-runtime added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-653c252a-c193-4aa0-9588-9d8467326dba;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.logging.log4j#log4j-api;2.10.0 in central\n",
      "\tfound org.apache.derby#derby;10.14.1.0 in central\n",
      "\tfound org.apache.hive#hive-metastore;3.1.3 in central\n",
      "\tfound org.apache.hive#hive-serde;3.1.3 in central\n",
      "\tfound org.apache.hive#hive-common;3.1.3 in central\n",
      "\tfound org.apache.hive#hive-classification;3.1.3 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.10 in central\n",
      "\tfound org.apache.hive#hive-upgrade-acid;3.1.3 in central\n",
      "\tfound org.apache.hive#hive-shims;3.1.3 in central\n",
      "\tfound org.apache.hive.shims#hive-shims-common;3.1.3 in central\n",
      "\tfound org.apache.logging.log4j#log4j-slf4j-impl;2.17.1 in central\n",
      "\tfound com.google.guava#guava;19.0 in central\n",
      "\tfound commons-lang#commons-lang;2.6 in central\n",
      "\tfound org.apache.thrift#libthrift;0.9.3 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound org.apache.zookeeper#zookeeper;3.4.6 in central\n",
      "\tfound org.slf4j#slf4j-log4j12;1.6.1 in central\n",
      "\tfound log4j#log4j;1.2.16 in central\n",
      "\tfound jline#jline;2.12 in central\n",
      "\tfound io.netty#netty;3.7.0.Final in central\n",
      "\tfound org.apache.logging.log4j#log4j-core;2.17.1 in central\n",
      "\tfound org.apache.hive.shims#hive-shims-0.23;3.1.3 in central\n",
      "\tfound org.apache.hadoop#hadoop-yarn-server-resourcemanager;3.1.0 in central\n",
      "\tfound javax.servlet#javax.servlet-api;3.1.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;3.1.0 in central\n",
      "\tfound com.google.inject.extensions#guice-servlet;4.0 in central\n",
      "\tfound com.google.inject#guice;4.0 in central\n",
      "\tfound javax.inject#javax.inject;1 in central\n",
      "\tfound aopalliance#aopalliance;1.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in central\n",
      "\tfound commons-io#commons-io;2.6 in central\n",
      "\tfound com.sun.jersey#jersey-json;1.19 in central\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
      "\tfound com.sun.xml.bind#jaxb-impl;2.2.3-1 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.11 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-jaxrs;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-xc;1.9.13 in central\n",
      "\tfound com.sun.jersey#jersey-core;1.19 in central\n",
      "\tfound javax.ws.rs#jsr311-api;1.1.1 in central\n",
      "\tfound com.sun.jersey.contribs#jersey-guice;1.19 in central\n",
      "\tfound com.sun.jersey#jersey-servlet;1.19 in central\n",
      "\tfound com.sun.jersey#jersey-server;1.19 in central\n",
      "\tfound org.apache.hadoop#hadoop-yarn-common;3.1.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-yarn-api;3.1.0 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.12.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-auth;3.1.0 in central\n",
      "\tfound com.nimbusds#nimbus-jose-jwt;4.41.1 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound net.minidev#json-smart;2.3 in central\n",
      "\tfound net.minidev#accessors-smart;1.2 in central\n",
      "\tfound org.ow2.asm#asm;5.0.4 in central\n",
      "\tfound org.apache.kerby#kerb-simplekdc;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-client;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-config;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-core;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-pkix;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-asn1;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-util;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-common;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-crypto;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-util;1.0.1 in central\n",
      "\tfound org.apache.kerby#token-provider;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-admin;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-server;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-identity;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-xdr;1.0.1 in central\n",
      "\tfound org.apache.commons#commons-compress;1.19 in central\n",
      "\tfound org.eclipse.jetty#jetty-util;9.3.19.v20170502 in central\n",
      "\tfound com.sun.jersey#jersey-client;1.19 in central\n",
      "\tfound commons-cli#commons-cli;1.2 in central\n",
      "\tfound log4j#log4j;1.2.17 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.12.0 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.12.0 in central\n",
      "\tfound com.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.12.0 in central\n",
      "\tfound jakarta.xml.bind#jakarta.xml.bind-api;2.3.2 in central\n",
      "\tfound jakarta.activation#jakarta.activation-api;1.2.1 in central\n",
      "\tfound com.fasterxml.jackson.jaxrs#jackson-jaxrs-json-provider;2.12.0 in central\n",
      "\tfound com.fasterxml.jackson.jaxrs#jackson-jaxrs-base;2.12.0 in central\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;9.3.19.v20170502 in central\n",
      "\tfound org.apache.hadoop#hadoop-yarn-server-common;3.1.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-yarn-registry;3.1.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-common;3.1.0 in central\n",
      "\tfound org.apache.commons#commons-math3;3.1.1 in central\n",
      "\tfound commons-net#commons-net;3.6 in central\n",
      "\tfound commons-collections#commons-collections;3.2.2 in central\n",
      "\tfound org.eclipse.jetty#jetty-server;9.3.20.v20170531 in central\n",
      "\tfound org.eclipse.jetty#jetty-http;9.3.20.v20170531 in central\n",
      "\tfound org.eclipse.jetty#jetty-io;9.3.20.v20170531 in central\n",
      "\tfound org.eclipse.jetty#jetty-servlet;9.3.20.v20170531 in central\n",
      "\tfound org.eclipse.jetty#jetty-security;9.3.20.v20170531 in central\n",
      "\tfound org.eclipse.jetty#jetty-webapp;9.3.20.v20170531 in central\n",
      "\tfound org.eclipse.jetty#jetty-xml;9.3.20.v20170531 in central\n",
      "\tfound commons-beanutils#commons-beanutils;1.9.3 in central\n",
      "\tfound org.apache.commons#commons-configuration2;2.1.1 in central\n",
      "\tfound org.apache.avro#avro;1.8.2 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.7 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.4 in central\n",
      "\tfound org.tukaani#xz;1.5 in central\n",
      "\tfound com.google.re2j#re2j;1.1 in central\n",
      "\tfound com.google.code.gson#gson;2.2.4 in central\n",
      "\tfound com.jcraft#jsch;0.1.54 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound org.codehaus.woodstox#stax2-api;3.1.4 in central\n",
      "\tfound com.fasterxml.woodstox#woodstox-core;5.0.3 in central\n",
      "\tfound commons-daemon#commons-daemon;1.0.13 in central\n",
      "\tfound dnsjava#dnsjava;2.1.7 in central\n",
      "\tfound org.fusesource.leveldbjni#leveldbjni-all;1.8 in central\n",
      "\tfound org.apache.geronimo.specs#geronimo-jcache_1.0_spec;1.0-alpha-1 in central\n",
      "\tfound org.ehcache#ehcache;3.3.1 in central\n",
      "\tfound com.zaxxer#HikariCP-java7;2.4.12 in central\n",
      "\tfound org.apache.hadoop#hadoop-yarn-server-applicationhistoryservice;3.1.0 in central\n",
      "\tfound de.ruedigermoeller#fst;2.50 in central\n",
      "\tfound com.cedarsoftware#java-util;1.9.0 in central\n",
      "\tfound com.cedarsoftware#json-io;2.5.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-yarn-server-web-proxy;3.1.0 in central\n",
      "\tfound javax.servlet.jsp#jsp-api;2.1 in central\n",
      "\tfound com.microsoft.sqlserver#mssql-jdbc;6.2.1.jre7 in central\n",
      "\tfound org.apache.hive.shims#hive-shims-scheduler;3.1.3 in central\n",
      "\tfound org.apache.hive#hive-storage-api;2.7.0 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.9 in central\n",
      "\tfound org.apache.orc#orc-core;1.5.8 in central\n",
      "\tfound org.apache.orc#orc-shims;1.5.8 in central\n",
      "\tfound io.airlift#aircompressor;0.10 in central\n",
      "\tfound org.eclipse.jetty#jetty-rewrite;9.3.20.v20170531 in central\n",
      "\tfound org.eclipse.jetty#jetty-client;9.3.20.v20170531 in central\n",
      "\tfound joda-time#joda-time;2.9.9 in central\n",
      "\tfound org.apache.logging.log4j#log4j-1.2-api;2.17.1 in central\n",
      "\tfound org.apache.logging.log4j#log4j-web;2.17.1 in central\n",
      "\tfound org.apache.ant#ant;1.9.1 in central\n",
      "\tfound org.apache.ant#ant-launcher;1.9.1 in central\n",
      "\tfound net.sf.jpam#jpam;1.1 in central\n",
      "\tfound com.tdunning#json;1.8 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;3.1.0 in central\n",
      "\tfound io.dropwizard.metrics#metrics-jvm;3.1.0 in central\n",
      "\tfound io.dropwizard.metrics#metrics-json;3.1.0 in central\n",
      "\tfound com.github.joshelser#dropwizard-metrics-hadoop-metrics2-reporter;0.1.2 in central\n",
      "\tfound javolution#javolution;5.5.1 in central\n",
      "\tfound org.apache.hive#hive-service-rpc;3.1.3 in central\n",
      "\tfound org.apache.thrift#libfb303;0.9.3 in central\n",
      "\tfound org.apache.arrow#arrow-vector;0.8.0 in central\n",
      "\tfound org.apache.arrow#arrow-format;0.8.0 in central\n",
      "\tfound com.vlkan#flatbuffers;1.2.0-3f79e055 in central\n",
      "\tfound org.apache.arrow#arrow-memory;0.8.0 in central\n",
      "\tfound io.netty#netty-buffer;4.1.17.Final in central\n",
      "\tfound io.netty#netty-common;4.1.17.Final in central\n",
      "\tfound com.carrotsearch#hppc;0.7.2 in central\n",
      "\tfound net.sf.opencsv#opencsv;2.3 in central\n",
      "\tfound org.apache.parquet#parquet-hadoop-bundle;1.10.0 in central\n",
      "\tfound org.apache.hive#hive-standalone-metastore;3.1.3 in central\n",
      "\tfound com.jolbox#bonecp;0.8.0.RELEASE in central\n",
      "\tfound com.zaxxer#HikariCP;2.6.1 in central\n",
      "\tfound commons-dbcp#commons-dbcp;1.4 in central\n",
      "\tfound commons-pool#commons-pool;1.5.4 in central\n",
      "\tfound org.antlr#antlr-runtime;3.5.2 in central\n",
      "\tfound org.datanucleus#datanucleus-api-jdo;4.2.4 in central\n",
      "\tfound org.datanucleus#datanucleus-core;4.1.17 in central\n",
      "\tfound org.datanucleus#datanucleus-rdbms;4.1.19 in central\n",
      "\tfound org.datanucleus#javax.jdo;3.2.0-m3 in central\n",
      "\tfound javax.transaction#transaction-api;1.1 in central\n",
      "\tfound sqlline#sqlline;1.3.0 in central\n",
      "\tfound org.apache.hbase#hbase-client;2.0.0-alpha4 in central\n",
      "\tfound org.apache.hbase.thirdparty#hbase-shaded-protobuf;1.0.1 in central\n",
      "\tfound org.apache.hbase#hbase-protocol-shaded;2.0.0-alpha4 in central\n",
      "\tfound org.apache.yetus#audience-annotations;0.5.0 in central\n",
      "\tfound junit#junit;4.11 in central\n",
      "\tfound org.hamcrest#hamcrest-core;1.3 in central\n",
      "\tfound org.apache.hbase#hbase-protocol;2.0.0-alpha4 in central\n",
      "\tfound org.apache.hbase.thirdparty#hbase-shaded-miscellaneous;1.0.1 in central\n",
      "\tfound org.apache.hbase.thirdparty#hbase-shaded-netty;1.0.1 in central\n",
      "\tfound org.apache.htrace#htrace-core;3.2.0-incubating in central\n",
      "\tfound org.jruby.jcodings#jcodings;1.0.18 in central\n",
      "\tfound org.jruby.joni#joni;2.1.11 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;3.2.1 in central\n",
      "\tfound org.apache.commons#commons-crypto;1.0.0 in central\n",
      "\tfound javax.jdo#jdo-api;3.0.1 in central\n",
      "\tfound javax.transaction#jta;1.1 in central\n",
      "\tfound co.cask.tephra#tephra-api;0.6.0 in central\n",
      "\tfound co.cask.tephra#tephra-core;0.6.0 in central\n",
      "\tfound com.google.inject.extensions#guice-assistedinject;3.0 in central\n",
      "\tfound it.unimi.dsi#fastutil;6.5.6 in central\n",
      "\tfound org.apache.twill#twill-common;0.6.0-incubating in central\n",
      "\tfound org.apache.twill#twill-core;0.6.0-incubating in central\n",
      "\tfound org.apache.twill#twill-api;0.6.0-incubating in central\n",
      "\tfound org.apache.twill#twill-discovery-api;0.6.0-incubating in central\n",
      "\tfound org.apache.twill#twill-zookeeper;0.6.0-incubating in central\n",
      "\tfound org.apache.twill#twill-discovery-core;0.6.0-incubating in central\n",
      "\tfound co.cask.tephra#tephra-hbase-compat-1.0;0.6.0 in central\n",
      "\tfound org.apache.hive#hive-exec;3.1.3 in central\n",
      "\tfound org.apache.hive#hive-llap-tez;3.1.3 in central\n",
      "\tfound org.apache.hive#hive-llap-client;3.1.3 in central\n",
      "\tfound org.apache.hive#hive-llap-common;3.1.3 in central\n",
      "\tfound org.antlr#ST4;4.0.4 in central\n",
      "\tfound org.apache.ivy#ivy;2.4.0 in central\n",
      "\tfound org.codehaus.groovy#groovy-all;2.4.11 in central\n",
      "\tfound org.apache.calcite#calcite-core;1.16.0 in central\n",
      "\tfound org.apache.calcite#calcite-linq4j;1.16.0 in central\n",
      "\tfound com.esri.geometry#esri-geometry-api;2.0.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.1 in central\n",
      "\tfound com.yahoo.datasketches#sketches-core;0.9.0 in central\n",
      "\tfound com.yahoo.datasketches#memory;0.9.0 in central\n",
      "\tfound org.codehaus.janino#janino;2.7.6 in central\n",
      "\tfound org.codehaus.janino#commons-compiler;2.7.6 in central\n",
      "\tfound org.apache.calcite.avatica#avatica;1.11.0 in central\n",
      "\tfound stax#stax-api;1.0.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.36 in central\n",
      ":: resolution report :: resolve 1973ms :: artifacts dl 42ms\n",
      "\t:: modules in use:\n",
      "\taopalliance#aopalliance;1.0 from central in [default]\n",
      "\tco.cask.tephra#tephra-api;0.6.0 from central in [default]\n",
      "\tco.cask.tephra#tephra-core;0.6.0 from central in [default]\n",
      "\tco.cask.tephra#tephra-hbase-compat-1.0;0.6.0 from central in [default]\n",
      "\tcom.carrotsearch#hppc;0.7.2 from central in [default]\n",
      "\tcom.cedarsoftware#java-util;1.9.0 from central in [default]\n",
      "\tcom.cedarsoftware#json-io;2.5.1 from central in [default]\n",
      "\tcom.esri.geometry#esri-geometry-api;2.0.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.12.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.12.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.12.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.jaxrs#jackson-jaxrs-base;2.12.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.jaxrs#jackson-jaxrs-json-provider;2.12.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.12.0 from central in [default]\n",
      "\tcom.fasterxml.woodstox#woodstox-core;5.0.3 from central in [default]\n",
      "\tcom.github.joshelser#dropwizard-metrics-hadoop-metrics2-reporter;0.1.2 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.1 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.2.4 from central in [default]\n",
      "\tcom.google.guava#guava;19.0 from central in [default]\n",
      "\tcom.google.inject#guice;4.0 from central in [default]\n",
      "\tcom.google.inject.extensions#guice-assistedinject;3.0 from central in [default]\n",
      "\tcom.google.inject.extensions#guice-servlet;4.0 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.1 from central in [default]\n",
      "\tcom.jcraft#jsch;0.1.54 from central in [default]\n",
      "\tcom.jolbox#bonecp;0.8.0.RELEASE from central in [default]\n",
      "\tcom.microsoft.sqlserver#mssql-jdbc;6.2.1.jre7 from central in [default]\n",
      "\tcom.nimbusds#nimbus-jose-jwt;4.41.1 from central in [default]\n",
      "\tcom.sun.jersey#jersey-client;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-core;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-json;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-server;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-servlet;1.19 from central in [default]\n",
      "\tcom.sun.jersey.contribs#jersey-guice;1.19 from central in [default]\n",
      "\tcom.sun.xml.bind#jaxb-impl;2.2.3-1 from central in [default]\n",
      "\tcom.tdunning#json;1.8 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.7 from central in [default]\n",
      "\tcom.vlkan#flatbuffers;1.2.0-3f79e055 from central in [default]\n",
      "\tcom.yahoo.datasketches#memory;0.9.0 from central in [default]\n",
      "\tcom.yahoo.datasketches#sketches-core;0.9.0 from central in [default]\n",
      "\tcom.zaxxer#HikariCP;2.6.1 from central in [default]\n",
      "\tcom.zaxxer#HikariCP-java7;2.4.12 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils;1.9.3 from central in [default]\n",
      "\tcommons-cli#commons-cli;1.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-collections#commons-collections;3.2.2 from central in [default]\n",
      "\tcommons-daemon#commons-daemon;1.0.13 from central in [default]\n",
      "\tcommons-dbcp#commons-dbcp;1.4 from central in [default]\n",
      "\tcommons-io#commons-io;2.6 from central in [default]\n",
      "\tcommons-lang#commons-lang;2.6 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tcommons-net#commons-net;3.6 from central in [default]\n",
      "\tcommons-pool#commons-pool;1.5.4 from central in [default]\n",
      "\tde.ruedigermoeller#fst;2.50 from central in [default]\n",
      "\tdnsjava#dnsjava;2.1.7 from central in [default]\n",
      "\tio.airlift#aircompressor;0.10 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;3.2.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-json;3.1.0 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-jvm;3.1.0 from central in [default]\n",
      "\tio.netty#netty;3.7.0.Final from central in [default]\n",
      "\tio.netty#netty-buffer;4.1.17.Final from central in [default]\n",
      "\tio.netty#netty-common;4.1.17.Final from central in [default]\n",
      "\tit.unimi.dsi#fastutil;6.5.6 from central in [default]\n",
      "\tjakarta.activation#jakarta.activation-api;1.2.1 from central in [default]\n",
      "\tjakarta.xml.bind#jakarta.xml.bind-api;2.3.2 from central in [default]\n",
      "\tjavax.inject#javax.inject;1 from central in [default]\n",
      "\tjavax.jdo#jdo-api;3.0.1 from central in [default]\n",
      "\tjavax.servlet#javax.servlet-api;3.1.0 from central in [default]\n",
      "\tjavax.servlet.jsp#jsp-api;2.1 from central in [default]\n",
      "\tjavax.transaction#jta;1.1 from central in [default]\n",
      "\tjavax.transaction#transaction-api;1.1 from central in [default]\n",
      "\tjavax.ws.rs#jsr311-api;1.1.1 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.11 from central in [default]\n",
      "\tjavolution#javolution;5.5.1 from central in [default]\n",
      "\tjline#jline;2.12 from central in [default]\n",
      "\tjoda-time#joda-time;2.9.9 from central in [default]\n",
      "\tjunit#junit;4.11 from central in [default]\n",
      "\tlog4j#log4j;1.2.17 from central in [default]\n",
      "\tnet.minidev#accessors-smart;1.2 from central in [default]\n",
      "\tnet.minidev#json-smart;2.3 from central in [default]\n",
      "\tnet.sf.jpam#jpam;1.1 from central in [default]\n",
      "\tnet.sf.opencsv#opencsv;2.3 from central in [default]\n",
      "\torg.antlr#ST4;4.0.4 from central in [default]\n",
      "\torg.antlr#antlr-runtime;3.5.2 from central in [default]\n",
      "\torg.apache.ant#ant;1.9.1 from central in [default]\n",
      "\torg.apache.ant#ant-launcher;1.9.1 from central in [default]\n",
      "\torg.apache.arrow#arrow-format;0.8.0 from central in [default]\n",
      "\torg.apache.arrow#arrow-memory;0.8.0 from central in [default]\n",
      "\torg.apache.arrow#arrow-vector;0.8.0 from central in [default]\n",
      "\torg.apache.avro#avro;1.8.2 from central in [default]\n",
      "\torg.apache.calcite#calcite-core;1.16.0 from central in [default]\n",
      "\torg.apache.calcite#calcite-linq4j;1.16.0 from central in [default]\n",
      "\torg.apache.calcite.avatica#avatica;1.11.0 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.19 from central in [default]\n",
      "\torg.apache.commons#commons-configuration2;2.1.1 from central in [default]\n",
      "\torg.apache.commons#commons-crypto;1.0.0 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.9 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.1.1 from central in [default]\n",
      "\torg.apache.derby#derby;10.14.1.0 from central in [default]\n",
      "\torg.apache.geronimo.specs#geronimo-jcache_1.0_spec;1.0-alpha-1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;3.1.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-auth;3.1.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-common;3.1.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-yarn-api;3.1.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-yarn-common;3.1.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-yarn-registry;3.1.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-yarn-server-applicationhistoryservice;3.1.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-yarn-server-common;3.1.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-yarn-server-resourcemanager;3.1.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-yarn-server-web-proxy;3.1.0 from central in [default]\n",
      "\torg.apache.hbase#hbase-client;2.0.0-alpha4 from central in [default]\n",
      "\torg.apache.hbase#hbase-protocol;2.0.0-alpha4 from central in [default]\n",
      "\torg.apache.hbase#hbase-protocol-shaded;2.0.0-alpha4 from central in [default]\n",
      "\torg.apache.hbase.thirdparty#hbase-shaded-miscellaneous;1.0.1 from central in [default]\n",
      "\torg.apache.hbase.thirdparty#hbase-shaded-netty;1.0.1 from central in [default]\n",
      "\torg.apache.hbase.thirdparty#hbase-shaded-protobuf;1.0.1 from central in [default]\n",
      "\torg.apache.hive#hive-classification;3.1.3 from central in [default]\n",
      "\torg.apache.hive#hive-common;3.1.3 from central in [default]\n",
      "\torg.apache.hive#hive-exec;3.1.3 from central in [default]\n",
      "\torg.apache.hive#hive-llap-client;3.1.3 from central in [default]\n",
      "\torg.apache.hive#hive-llap-common;3.1.3 from central in [default]\n",
      "\torg.apache.hive#hive-llap-tez;3.1.3 from central in [default]\n",
      "\torg.apache.hive#hive-metastore;3.1.3 from central in [default]\n",
      "\torg.apache.hive#hive-serde;3.1.3 from central in [default]\n",
      "\torg.apache.hive#hive-service-rpc;3.1.3 from central in [default]\n",
      "\torg.apache.hive#hive-shims;3.1.3 from central in [default]\n",
      "\torg.apache.hive#hive-standalone-metastore;3.1.3 from central in [default]\n",
      "\torg.apache.hive#hive-storage-api;2.7.0 from central in [default]\n",
      "\torg.apache.hive#hive-upgrade-acid;3.1.3 from central in [default]\n",
      "\torg.apache.hive.shims#hive-shims-0.23;3.1.3 from central in [default]\n",
      "\torg.apache.hive.shims#hive-shims-common;3.1.3 from central in [default]\n",
      "\torg.apache.hive.shims#hive-shims-scheduler;3.1.3 from central in [default]\n",
      "\torg.apache.htrace#htrace-core;3.2.0-incubating from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.apache.ivy#ivy;2.4.0 from central in [default]\n",
      "\torg.apache.kerby#kerb-admin;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-client;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-common;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-core;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-crypto;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-identity;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-server;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-simplekdc;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-util;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-asn1;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-config;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-pkix;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-util;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-xdr;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#token-provider;1.0.1 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-1.2-api;2.17.1 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-api;2.10.0 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-core;2.17.1 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-slf4j-impl;2.17.1 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-web;2.17.1 from central in [default]\n",
      "\torg.apache.orc#orc-core;1.5.8 from central in [default]\n",
      "\torg.apache.orc#orc-shims;1.5.8 from central in [default]\n",
      "\torg.apache.parquet#parquet-hadoop-bundle;1.10.0 from central in [default]\n",
      "\torg.apache.thrift#libfb303;0.9.3 from central in [default]\n",
      "\torg.apache.thrift#libthrift;0.9.3 from central in [default]\n",
      "\torg.apache.twill#twill-api;0.6.0-incubating from central in [default]\n",
      "\torg.apache.twill#twill-common;0.6.0-incubating from central in [default]\n",
      "\torg.apache.twill#twill-core;0.6.0-incubating from central in [default]\n",
      "\torg.apache.twill#twill-discovery-api;0.6.0-incubating from central in [default]\n",
      "\torg.apache.twill#twill-discovery-core;0.6.0-incubating from central in [default]\n",
      "\torg.apache.twill#twill-zookeeper;0.6.0-incubating from central in [default]\n",
      "\torg.apache.yetus#audience-annotations;0.5.0 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper;3.4.6 from central in [default]\n",
      "\torg.codehaus.groovy#groovy-all;2.4.11 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-jaxrs;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-xc;1.9.13 from central in [default]\n",
      "\torg.codehaus.janino#commons-compiler;2.7.6 from central in [default]\n",
      "\torg.codehaus.janino#janino;2.7.6 from central in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
      "\torg.codehaus.woodstox#stax2-api;3.1.4 from central in [default]\n",
      "\torg.datanucleus#datanucleus-api-jdo;4.2.4 from central in [default]\n",
      "\torg.datanucleus#datanucleus-core;4.1.17 from central in [default]\n",
      "\torg.datanucleus#datanucleus-rdbms;4.1.19 from central in [default]\n",
      "\torg.datanucleus#javax.jdo;3.2.0-m3 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-client;9.3.20.v20170531 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-http;9.3.20.v20170531 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-io;9.3.20.v20170531 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-rewrite;9.3.20.v20170531 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-security;9.3.20.v20170531 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-server;9.3.20.v20170531 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-servlet;9.3.20.v20170531 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.3.19.v20170502 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.3.19.v20170502 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-webapp;9.3.20.v20170531 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-xml;9.3.20.v20170531 from central in [default]\n",
      "\torg.ehcache#ehcache;3.3.1 from central in [default]\n",
      "\torg.fusesource.leveldbjni#leveldbjni-all;1.8 from central in [default]\n",
      "\torg.hamcrest#hamcrest-core;1.3 from central in [default]\n",
      "\torg.jruby.jcodings#jcodings;1.0.18 from central in [default]\n",
      "\torg.jruby.joni#joni;2.1.11 from central in [default]\n",
      "\torg.ow2.asm#asm;5.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 from central in [default]\n",
      "\torg.slf4j#slf4j-log4j12;1.6.1 from central in [default]\n",
      "\torg.tukaani#xz;1.5 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\tsqlline#sqlline;1.3.0 from central in [default]\n",
      "\tstax#stax-api;1.0.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.slf4j#slf4j-api;1.7.10 by [org.slf4j#slf4j-api;1.7.36] in [default]\n",
      "\tlog4j#log4j;1.2.16 by [log4j#log4j;1.2.17] in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 by [commons-logging#commons-logging;1.2] in [default]\n",
      "\torg.apache.commons#commons-lang3;3.4 by [org.apache.commons#commons-lang3;3.9] in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.4 by [org.xerial.snappy#snappy-java;1.1.8.2] in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 by [com.google.code.findbugs#jsr305;3.0.1] in [default]\n",
      "\tcommons-logging#commons-logging;1.0.4 by [commons-logging#commons-logging;1.2] in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;3.1.0 by [io.dropwizard.metrics#metrics-core;3.2.1] in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;3.1.2 by [io.dropwizard.metrics#metrics-core;3.1.0] in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 by [com.google.code.findbugs#jsr305;3.0.1] in [default]\n",
      "\torg.apache.commons#commons-lang3;3.2 by [org.apache.commons#commons-lang3;3.9] in [default]\n",
      "\torg.apache.commons#commons-lang3;3.6 by [org.apache.commons#commons-lang3;3.9] in [default]\n",
      "\tcom.google.inject#guice;3.0 by [com.google.inject#guice;4.0] in [default]\n",
      "\tcom.google.code.findbugs#jsr305;2.0.1 by [com.google.code.findbugs#jsr305;3.0.0] in [default]\n",
      "\tcom.google.guava#guava;14.0.1 by [com.google.guava#guava;19.0] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |  224  |   0   |   0   |   15  ||  209  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-653c252a-c193-4aa0-9588-9d8467326dba\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 209 already retrieved (0kB/14ms)\n",
      "Hive Session ID = e9088c3f-9a48-4a8b-8dfd-734c14951b69\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[event_time: timestamp, event_type: string, product_id: int, category_id: bigint, category_code: string, brand: string, price: float, user_id: int, user_session: string, event_date: date]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a few helpful methods for setting the local context\n",
    "# which database like `use that_database` in SQL\n",
    "# with unity catalog (spark.catalog.setCurrentCatalog....)\n",
    "spark.catalog.setCurrentDatabase(\"default\")\n",
    "spark.catalog.listTables()\n",
    "spark.catalog.tableExists(dl_table_name)\n",
    "spark.table(dl_managed_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6598b8a9-bfb1-49aa-86c9-cc9b1b67f5b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "> Note: If you see `java.sql.SQLException: Failed to start database 'metastore_db' with class loader jdk.internal.loader.ClassLoaders$AppClassLoader...` then you need to detach the `kernel` from the other notebook you have open. You can only have one notebook running with the local Metastore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd9a92d-0b69-4131-9a17-ccbfe76f97fe",
   "metadata": {},
   "source": [
    "## Successful Streaming Begins with Metadata (lots and lots of metadata)\n",
    "> In other words, if you don't understand how the table is laid out, what the structure of the table is (columns, types, is the table narrow or wide? do you know what any of the columns actually are?\n",
    "\n",
    "Remember, when in lost or in doubt, always consult the data (metadata). To Peek at the Table Metadata with `detail()`\n",
    "* - Use `DeltaTable.forName(spark, 'catalog.schema.table|schema.table|table').detail()` \n",
    "* - or `DeltaTable.forPath(spark, '/path/to/table/).detail()` for Unmanaged tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5075440a-f931-40ea-886e-4249cd10b806",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Starting Small (Baby Steps)\n",
    "\n",
    "dt_ecomm = DeltaTable.forName(spark, dl_managed_table)\n",
    "table_details = dt_ecomm.detail()\n",
    "\n",
    "# go on, take a peek (no one's looking)\n",
    "table_details.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcfbfd3-2669-4451-9dcf-a0f21b621e2f",
   "metadata": {},
   "source": [
    "### Table Details. Providing you with all the ... well details\n",
    "Scanning the StructType of the `detail()` dataframe gives you a lot of data. The following use cases can be solved with the metadata:\n",
    "\n",
    "```\n",
    "root\n",
    " |-- format: string (nullable = true)\n",
    " |-- id: string (nullable = true)\n",
    " |-- name: string (nullable = true)\n",
    " |-- description: string (nullable = true)\n",
    " |-- location: string (nullable = true)\n",
    " |-- createdAt: timestamp (nullable = true)\n",
    " |-- lastModified: timestamp (nullable = true)\n",
    " |-- partitionColumns: array (nullable = true)\n",
    " |    |-- element: string (containsNull = true)\n",
    " |-- numFiles: long (nullable = true)\n",
    " |-- sizeInBytes: long (nullable = true)\n",
    " |-- properties: map (nullable = true)\n",
    " |    |-- key: string\n",
    " |    |-- value: string (valueContainsNull = true)\n",
    " |-- minReaderVersion: integer (nullable = true)\n",
    " |-- minWriterVersion: integer (nullable = true)\n",
    " |-- tableFeatures: array (nullable = true)\n",
    " |    |-- element: string (containsNull = true)\n",
    "```\n",
    "\n",
    "1. **Calculate Table Freshness**: `abs(current_time()-{table.lastModified})`: To answer the universal question of - \"How Fresh Is It?\".\n",
    "2. **How Fast is the Table Growing?**: Size does matter. If we have two tables, tableA is 100gb and has `createdAt` of one year ago, and tableB is 100gb and was created yesterday, then we've got a scalability monster. Using the `freshness` technique, you can calculate the `days` a table has `existed`, and calculate the `avg` bytes per day using `sizeInBytes`.\n",
    "3. **What is the Table Telling Us?**: Using the `properties` map, we can easily view ALL Table Properties, including those used to `automate` Delta Lake like `delta.logRetentionDuration` or those *we bring to the table* - pun truly intended. Like `catalog.team_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c3eb912-19c2-4354-8abf-954701bde8eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+----------+-----------------------+-----------------------+---------+---------+\n",
      "|now                       |todaysDate|createdAt              |lastModified           |ageInDays|staleDays|\n",
      "+--------------------------+----------+-----------------------+-----------------------+---------+---------+\n",
      "|2024-06-01 23:44:55.156827|2024-06-01|2024-06-01 23:25:12.682|2024-06-01 23:40:35.782|0        |0        |\n",
      "+--------------------------+----------+-----------------------+-----------------------+---------+---------+\n",
      "\n",
      "\n",
      "Don't Panic!\n",
      "\n",
      "The table 'spark_catalog.default.ecomm_by_day' has a known classification of 'all-access'.\n",
      "\n",
      "The table is owned by the following team 'dldg_authors'.\n",
      "\n",
      "If we need contact them via slack @ https://delta-users.slack.com/archives/CG9LR6LN4\n"
     ]
    }
   ],
   "source": [
    "# Feel Free to Mess with the following cell to get used to the data available to you about the ecomm_by_day table.\n",
    "from pyspark.sql.functions import col, current_timestamp, to_date, datediff\n",
    "tbl_dets = (\n",
    "    table_details\n",
    "    .withColumn(\"now\", current_timestamp())\n",
    "    .withColumn(\"todaysDate\", to_date(col(\"now\")))\n",
    "    .withColumn(\"ageInDays\", datediff(col(\"todaysDate\"),to_date(\"createdAt\")))\n",
    "    .withColumn(\"staleDays\", datediff(col(\"todaysDate\"),to_date(\"lastModified\")))\n",
    ")\n",
    "# view all the time-based info on the table.\n",
    "(tbl_dets\n",
    " .select(\n",
    "     \"now\",\n",
    "     \"todaysDate\",\n",
    "     \"createdAt\",\n",
    "     \"lastModified\",\n",
    "     \"ageInDays\",\n",
    "     \"staleDays\")\n",
    " .show(truncate=False)\n",
    ")\n",
    "\n",
    "# fetch the dataframe as a local Row\n",
    "dets = tbl_dets.first()\n",
    "# see it's a Row...<class 'pyspark.sql.types.Row'>\n",
    "#print(type(dets))\n",
    "team_name = dets['properties']['catalog.team_name']\n",
    "team_slack = dets['properties']['catalog.engineering.comms.slack']\n",
    "table_classifiction = dets['properties']['catalog.table.classification']\n",
    "\n",
    "# stick to the details\n",
    "print(f\"\"\"\n",
    "Don't Panic!\\n\n",
    "The table '{dets.name}' has a known classification of '{table_classifiction}'.\\n\n",
    "The table is owned by the following team '{team_name}'.\\n\n",
    "If we need contact them via slack @ {team_slack}\n",
    "\"\"\")\n",
    "\n",
    "# or remember not to panic, everything is under control\n",
    "#print(f\"\"\"\n",
    "#I am no longer panicking.\\n \n",
    "#Why you ask?\\n\n",
    "#I know that I can count on {team_name} to deliver gold data, otherwise...\\n\n",
    "#to slack ({team_slack}) we ride questions in hand about the TABLE {dets.name}.\\n\n",
    "#Which happened to be created on {dets.createdAt} and last updated at {dets.lastModified}...\n",
    "#\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c57779-c0a0-4ad8-8810-0a335719ca09",
   "metadata": {},
   "source": [
    "## Inspecting the Volume, Size, and Charateristics of a Delta Table\n",
    "> there are many uses for math in a career as a data engineer. One of them is back of the envelope (or mostly right maths!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1474d650-5a72-47ea-9214-b327b0a75da8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------------+\n",
      "|numFiles|TableSizeInMegaBytes|avgMBPerFile|\n",
      "+--------+--------------------+------------+\n",
      "|       0|                 0.0|        NULL|\n",
      "+--------+--------------------+------------+\n"
     ]
    }
   ],
   "source": [
    "bytesToMB = 1000000\n",
    "(tbl_dets\n",
    " .select(\n",
    "     col(\"numFiles\"),\n",
    "     (col(\"sizeInBytes\")/bytesToMB)\n",
    "       .alias(\"TableSizeInMegaBytes\"),\n",
    "     ((col(\"sizeInBytes\")/bytesToMB)/col(\"numFiles\"))\n",
    "       .alias(\"avgMBPerFile\")\n",
    " ).show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0fae51-ea43-4a0d-83ee-b2b4fb2fdfc2",
   "metadata": {},
   "source": [
    "## What We've Learned about the Dataset\n",
    "> Note: The following information is based on the 'complete' ecomm dataset. The full 15gb csv. 807mb is the size on disk after zstd compression and Delta encoding. \n",
    "\n",
    "1. The naive average megabytes per file is around `17mb`. If you run `ls -lh` across any given day, you'll see more of an odd split between say 3mb and 18mb due to non optimized, non-bin backed table data on disk.\n",
    "    - (or the very *improbably* you may see exactly the value `0.0058685MB` if you are using the ***-sm*** dataset)\n",
    "2. There are `142` files taking up a `~2.4gb` for the `entire` table.\n",
    "    - (or even more improbably exactly `4` files taking up `0.023474MB` for the whole table)\n",
    "3. There are probably many more `rows` of data in the table, so if we wanted to get a 'quick' count, then that would be a good idea too. That can give us more `approximate` math to work with (rows/day) - even if we are off - we are better informed with approximate math than wild guesses and hopes and dreams.\n",
    "    - (unless we are looking for Magrathea or being attacked by Vorgons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de32047a-73c9-46fc-9858-178cbf1bab06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Table has 0 rows.\n",
      "\n",
      "*Daily Rows of 0\n",
      "\n",
      "Total Partitions in Table 0\n",
      "\n",
      "*Avg Files per Partition 0\n",
      "\n",
      "*Avg Row Size 0 in Bytes\n",
      "\n",
      "*Avg Rows per Delta Lake File: 0\n",
      "\n",
      "Records per Second: 0.0\n",
      "\n",
      "Records per Hour: 0.0\n"
     ]
    }
   ],
   "source": [
    "# convert the DeltaTable reference to a DataFrame\n",
    "seconds_in_day = 86400\n",
    "dt_as_df = dt_ecomm.toDF()\n",
    "total_rows = dt_as_df.count()\n",
    "\n",
    "# calculate the total number of partitions\n",
    "# cheating since we are just taking the first of many (or fail if none - we know it is event_date, but still)\n",
    "partitionCol = (dt_ecomm\n",
    "                .detail()\n",
    "                .first()[\"partitionColumns\"][0])\n",
    "total_partitions = (dt_as_df\n",
    "                    .select(col(partitionCol))\n",
    "                    .distinct()\n",
    "                    .count())\n",
    "avg_files_per_partition = dets['numFiles']/total_partitions if total_partitions != 0 else 0\n",
    "rows_per_day = total_rows/dets['numFiles'] if dets['numFiles'] != 0 else 0\n",
    "avg_row_size = dets['sizeInBytes']/total_rows if total_rows !=0 else 0\n",
    "\n",
    "# the * denotes maybe accurate, maybe really off. \n",
    "# - This is mostly cheap approximations (and maybe accurate math)\n",
    "print(f\"\"\"\n",
    "The Table has {total_rows} rows.\\n\n",
    "*Daily Rows of {rows_per_day}\\n\n",
    "Total Partitions in Table {total_partitions}\\n\n",
    "*Avg Files per Partition {avg_files_per_partition}\\n\n",
    "*Avg Row Size {avg_row_size} in Bytes\\n\n",
    "*Avg Rows per Delta Lake File: {rows_per_day/avg_files_per_partition if avg_files_per_partition != 0 else 0}\\n\n",
    "Records per Second: {rows_per_day/seconds_in_day}\\n\n",
    "Records per Hour: {rows_per_day/24}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301bd253-cb85-4fc6-a278-30cbe11d3256",
   "metadata": {},
   "source": [
    "# Our First Delta Lake Streaming Operation\n",
    "> Clap your Hands! Or Celebrate However you want. It's time to be Streaming\n",
    "\n",
    "Because we have potentially a gigantic amount of data - (Or depending on the adventure you chose a smaller set of 60, yes it should have been 42, but time...) - regardless, it is time to create our first streaming application.\n",
    "\n",
    "## What We'll Need\n",
    "1. A Place to Store our Application Metadata. Luckily we have our Local File Sytem, so we can just store the application data there for now. (See [common application directory](../../applications/README.md) to understand a little more.\n",
    "2. A [Way of Restricting the Volume of Data We Read](https://docs.delta.io/latest/delta-streaming.html#limit-input-rate)\n",
    "3. A [Means of Ignoring Things](https://docs.databricks.com/structured-streaming/delta-lake.html#ignore-updates-and-deletes) we don't currently care about.\n",
    "3. A Way of Limiting the Frequency in which our Application Runs (just like we want to limit the volume of data, when we start learning how to work with Streaming Data, it is better to slowly increase the rate which we will learn how to do.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb2ca66b-c1d7-4df1-be63-2685949069e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#spark.sql(\"drop table default.ecomm_aggs_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c78ea8fe-c4ab-48cd-b3e6-65aa4fadf7a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint_path=../../applications/dl_streaming_aggs/v0.0.1/_checkpoints\n",
      "root\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- category_id: long (nullable = true)\n",
      " |-- category_code: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: float (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      " |-- event_date: date (nullable = true)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/01 23:48:58 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`default`.`ecomm_aggs_table` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "24/06/01 23:48:58 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "# read from the `default.ecomm_by_day` table, modify the read options to limit the maxFilesPerTrigger\n",
    "# read up to 4 files, do a simple projection (select colA, colB)\n",
    "# write out to a new Delta Lake table. \n",
    "# Checkpoint the progress so we can `pick up where we left off`\n",
    "\n",
    "app_name = \"dl_streaming_aggs\"\n",
    "app_version = \"v0.0.1\"\n",
    "checkpoint_dir = \"../../applications\"\n",
    "checkpoint_path = f\"{checkpoint_dir}/{app_name}/{app_version}/_checkpoints\"\n",
    "print(f\"checkpoint_path={checkpoint_path}\")\n",
    "ecomm_aggs_table = 'default.ecomm_aggs_table'\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"32\")\n",
    "# create the streaming Delta source object\n",
    "ecomm_by_day_limited = (\n",
    "    spark.readStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"maxFilesPerTrigger\", 4)\n",
    "    .option(\"ignoreChanges\", True)\n",
    "    .option(\"withEventTimeOrder\", True)\n",
    "    .table(dl_managed_table)\n",
    ")\n",
    "\n",
    "# view the schema for the table (since we know everything else about it now too)\n",
    "ecomm_by_day_limited.printSchema()\n",
    "\n",
    "# next select the columns we care about (feel free to switch things up here too)\n",
    "ecomm_aggs = (\n",
    "    ecomm_by_day_limited\n",
    "    .withWatermark(\"event_time\", '10 minutes')\n",
    "    .select(\"event_time\", \"event_type\", \"product_id\", \"user_session\", \"user_id\", \"event_date\")\n",
    "    .groupBy(window(\"event_time\", \"30 minutes\"), \"user_id\", \"product_id\", \"event_date\")\n",
    "    .agg(count(\"event_type\").alias('session_events'))\n",
    ")\n",
    "\n",
    "# next create the streaming sink\n",
    "\n",
    "streamingQuery = (\n",
    "    ecomm_aggs.writeStream\n",
    "    .format(\"delta\")\n",
    "    .queryName(\"ecomm_aggs\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .outputMode(\"append\")\n",
    "    .partitionBy(\"event_date\")\n",
    "    .option(\"overwriteSchema\", False)\n",
    "    # triggers allow us to control the frequency in which a job will run. \n",
    "    # For the java nerds (me included) triggers run like scheduledThreadPools when using `processingTime` \n",
    "    # and once, will fire once and then the job will complete.\n",
    "    #.trigger(processingTime='42 seconds')\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(ecomm_aggs_table)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fae0347-0f25-49d3-90f9-9e5ca8977281",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Controlling the StreamingQuery\n",
    "1. We returned a `streamingQuery` object when we executed the last cell before. The Streaming Query object provides you with a gateway into the realtime metrics and behavior of your Delta-Spark based application performance.\n",
    "\n",
    "2. Given the application is `triggering` every `30s` that means twice a minute we'll have more data, as the job slowly chews through the 72 files of the data set, pulling in 600k files per tick.\n",
    "\n",
    "Take a look at the metadata provided to you by the `streamingQuery`. Think about how impressive the numbers are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f5c9735-e6e2-4354-99ad-f252b0b6a8d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'd0196677-96ef-46d2-869a-fd428e0bbaed',\n",
       " 'runId': '54829c1d-8029-40c3-b128-f78e644b292c',\n",
       " 'name': 'ecomm_aggs',\n",
       " 'timestamp': '2024-06-01T23:48:59.030Z',\n",
       " 'batchId': 0,\n",
       " 'numInputRows': 0,\n",
       " 'inputRowsPerSecond': 0.0,\n",
       " 'processedRowsPerSecond': 0.0,\n",
       " 'durationMs': {'latestOffset': 770, 'triggerExecution': 779},\n",
       " 'eventTime': {'watermark': '1970-01-01T00:00:00.000Z'},\n",
       " 'stateOperators': [],\n",
       " 'sources': [{'description': 'DeltaSource[file:/opt/spark/work-dir/hitchhikers_guide/warehouse/ecomm_by_day]',\n",
       "   'startOffset': None,\n",
       "   'endOffset': None,\n",
       "   'latestOffset': None,\n",
       "   'numInputRows': 0,\n",
       "   'inputRowsPerSecond': 0.0,\n",
       "   'processedRowsPerSecond': 0.0}],\n",
       " 'sink': {'description': 'DeltaSink[file:/opt/spark/work-dir/hitchhikers_guide/warehouse/ecomm_aggs_table]',\n",
       "  'numOutputRows': -1}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamingQuery.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e767e750-2019-4683-b3ce-4343b6ffd015",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input_rows_a_second:0.0\n",
      "\n",
      "processed_rows_a_second: 0.0\n"
     ]
    }
   ],
   "source": [
    "lprog = streamingQuery.lastProgress\n",
    "input_rows_sec = lprog['inputRowsPerSecond']\n",
    "processed_rows_sec = lprog['processedRowsPerSecond']\n",
    "\n",
    "print(f\"\"\"\n",
    "input_rows_a_second:{input_rows_sec}\\n\n",
    "processed_rows_a_second: {processed_rows_sec}\\n\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b994d389-df47-4a30-b344-831e3da1a9b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "^^ The prior output from the StreamingQueryListener is an aggregation of the collected runtime metadata, and statistical\n",
    "behavior captured during the last microBatch. You'll notice that we started on index 16, and endingOffset was 17.\n",
    "\n",
    "# Viewing the Delta Lake Information in the Streaming Query Stats\n",
    "```\n",
    "'startOffset': {\n",
    "  'sourceVersion': 1,\n",
    "  'reservoirId': '027b3701-5c07-46d4-9d96-e5539f81e8bf',\n",
    "  'reservoirVersion': 33,\n",
    "  'index': 16,\n",
    "  'isStartingVersion': True},\n",
    "'endOffset': {\n",
    "  'sourceVersion': 1,\n",
    "  'reservoirId': '027b3701-5c07-46d4-9d96-e5539f81e8bf',\n",
    "  'reservoirVersion': 33,\n",
    "  'index': 17,\n",
    "  'isStartingVersion': True\n",
    "}\n",
    "```\n",
    "\n",
    "This means we can take a look at the operations in the `/_checkpoints/offsets/17` directory. \n",
    "\n",
    "```\n",
    "v1\n",
    "{\"batchWatermarkMs\":1570578599000,\"batchTimestampMs\":1687853100013,\"conf\":{\"spark.sql.streaming.stateStore.providerClass\":\"org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider\",\"spark.sql.streaming.join.stateFormatVersion\":\"2\",\"spark.sql.streaming.stateStore.compression.codec\":\"lz4\",\"spark.sql.streaming.stateStore.rocksdb.formatVersion\":\"5\",\"spark.sql.streaming.statefulOperator.useStrictDistribution\":\"true\",\"spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion\":\"2\",\"spark.sql.streaming.multipleWatermarkPolicy\":\"min\",\"spark.sql.streaming.aggregation.stateFormatVersion\":\"2\",\"spark.sql.shuffle.partitions\":\"200\"}}\n",
    "{\"sourceVersion\":1,\"reservoirId\":\"027b3701-5c07-46d4-9d96-e5539f81e8bf\",\"reservoirVersion\":33,\"index\":17,\"isStartingVersion\":true}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c85a0a7-513e-42f8-82b4-a279b6f0d9b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamingQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c02d514-5320-4e1d-abfc-1ef6497ab69b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "streamingQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e9d7ff-acb3-4450-b718-ca434ec7aca8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Applications have State in the form of Checkpoints. \n",
    "> Delta maintains its state in the terms of completed atomic transactions.\n",
    "\n",
    "The application checkpoints track where the application has last successfully read from the Delta Lake table (source), and the application also keeps track of the delta version based on the resulting transformation and insert into the (sink). In our case we read from the `default.ecomm_by_day` and did some windowed aggregations for events per session, and then recorded the results in a new table named `default.ecomm_aggs_table`.\n",
    "\n",
    "Let's peak at the checkpoint data. Open up `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f8787b-836c-4101-a126-279de702238d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## View the Checkpoint Data\n",
    "> when you are managing a streaming application you will need to be familiar with both the Delta Log, as well as your application's own 'transaction' history which is stored in the 'checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "611cc35e-a070-4f2a-b084-6d068fbb7d9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4\n",
      "drwxr-xr-x 2 NBuser NBuser 64 Jun  1 23:48 commits\n",
      "-rw-r--r-- 1 NBuser NBuser 45 Jun  1 23:48 metadata\n",
      "drwxr-xr-x 2 NBuser NBuser 64 Jun  1 23:48 offsets\n",
      "\n",
      "echo \"---\"\n",
      "echo \"TOTAL FILES IN CHECKPOINT DIRECTORY\"\n",
      "# view the total number of commits in the commits dir\n",
      "ls -l ../../applications/dl_streaming_aggs/v0.0.1/_checkpoints/commits/ | wc -l\n",
      "\n",
      "# cat ../../applications/dl_streaming_aggs/v0.0.1/_checkpoints/metadata \n",
      "# ls -l ../../applications/dl_streaming_aggs/v0.0.1/_checkpoints/offsets\n",
      "#cat ../../applications/dl_streaming_aggs/v0.0.1/_checkpoints/offsets/1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls -l ../../applications/dl_streaming_aggs/v0.0.1/_checkpoints/\n",
    "# ls -l ../../applications/dl_streaming_aggs/v0.0.1/_checkpoints/commits/\n",
    "# get the last modifed file in the dir (limit to 1) - this is the last commit version (microbatch number - for structured streaming)\n",
    "\n",
    "# find the latest checkpoint in the checkpoints directory for the streaming application\n",
    "unset -v last_checkpoint\n",
    "for file in \"../../applications/dl_streaming_aggs/v0.0.1/_checkpoints/commits\"/*; do\n",
    "  [[ $file -nt $last_checkpoint ]] && last_checkpoint=$file\n",
    "done\n",
    "\n",
    "echo \"${last_checkpoint}\"\n",
    "# view the commit info\n",
    "cat $last_checkpoint\n",
    "echo \"---\"\n",
    "echo \"TOTAL FILES IN CHECKPOINT DIRECTORY\"\n",
    "# view the total number of commits in the commits dir\n",
    "ls -l ../../applications/dl_streaming_aggs/v0.0.1/_checkpoints/commits/ | wc -l\n",
    "\n",
    "# cat ../../applications/dl_streaming_aggs/v0.0.1/_checkpoints/metadata \n",
    "# ls -l ../../applications/dl_streaming_aggs/v0.0.1/_checkpoints/offsets\n",
    "#cat ../../applications/dl_streaming_aggs/v0.0.1/_checkpoints/offsets/1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce70c11e-dcbb-4c49-a1cc-cb051d90ce59",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The Fruits of our Quick Labor\n",
    "The shopping aggregations is our own 'sessionization' based on things that would work for the hitchhikers guide to Delta Lake streaming. Have we learned a lot from the data? Maybe. Have we learned a lot more about how Delta Lake works? Surely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3f2b354-6637-4a54-b30f-d1672f39fa59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+----------+--------------+\n",
      "|window|user_id|product_id|event_date|session_events|\n",
      "+------+-------+----------+----------+--------------+\n",
      "+------+-------+----------+----------+--------------+\n"
     ]
    }
   ],
   "source": [
    "(spark.read\n",
    " .table(\"default.ecomm_aggs_table\")\n",
    " .where(col(\"event_date\").isin(\"2019-10-01\",\"2019-10-02\"))\n",
    " .show(10, truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd05bd4-7963-4c40-8e08-b9df76a296ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extra Homework: Finding Neat Patterns in the Data\n",
    "> shopping is fun. We all do it, some of us even enjoy it. Regardless of your style, the one thing we have in common is that not one of us really shops the same. Investigate the 42 million shopping data points from this dataset to understand how people are shopping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a68175a-e3ca-4181-949f-331b041ed2bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------------+-------+\n",
      "|event_time|event_type|product_id|user_session|user_id|\n",
      "+----------+----------+----------+------------+-------+\n",
      "+----------+----------+----------+------------+-------+\n"
     ]
    }
   ],
   "source": [
    "(spark.read\n",
    " .table(dl_managed_table)\n",
    " .select(\"event_time\", \"event_type\", \"product_id\", \"user_session\", \"user_id\")\n",
    " .show(50, truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5cb4f97-dcfb-4b49-a523-5eaeca53642e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+-------+------------+\n",
      "|event_time|event_type|product_id|user_id|user_session|\n",
      "+----------+----------+----------+-------+------------+\n",
      "+----------+----------+----------+-------+------------+\n"
     ]
    }
   ],
   "source": [
    "# find a user who has an interesting shopping pattern\n",
    "# this user comes back frequently, views, comes back, and 10 days from the first\n",
    "# view finally makes a purchase\n",
    "\n",
    "(spark.read\n",
    " .table(dl_managed_table)\n",
    " .select(\"event_time\", \"event_type\", \"product_id\", \"user_id\", \"user_session\")\n",
    " .where(col(\"user_id\").eqNullSafe(516224384))\n",
    " .show(50, truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1026cfd7-e080-41ca-aaf4-d28644ff8b53",
   "metadata": {},
   "source": [
    "# Cleaning up with Vacuum.\n",
    "We are done with the introduction to Streaming. The First steps covers creating tables, and modifying the table properties, as well as understanding a little more about the structure of a Delta Lake table. During normal processing, you most likely overwrote, or deleted some data, for each transaction that affects the data in a given Delta Lake table, there are some artifacts (call it orphaned data or files) that are no longer needed for the *CURRENT* version of the Delta Lake table. We will learn more about using `vacuum` while preserving enough history to `undo`, `rewind`, or `time-travel` to a particular point in Table Time under "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec289a34-e8b0-4c76-b9c5-eace8ef765d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 0 files and directories in a total of 1 directories.\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"false\")\n",
    "DeltaTable.forName(spark, ecomm_aggs_table).vacuum(retentionHours=0)\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27270b28-039e-4ccc-8220-df5111d7955b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
