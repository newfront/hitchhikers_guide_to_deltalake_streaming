{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c40b19c7-0434-41cf-86cd-a153073577f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run first. then have fun.\n",
    "from pyspark.sql.functions import col\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# keep the default compression codec as zstd\n",
    "spark.conf.set(\"spark.sql.parquet.compression.codec\", \"zstd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdf07ce-0b60-438c-99ed-e0bb25f76fc5",
   "metadata": {},
   "source": [
    "# First Steps: Delta Lake Streaming\n",
    "> Note: This notebook relies heavily on the Apache Spark ecosystem. In the future we will have rust driven notebooks under `first steps` as well. \n",
    "\n",
    "We will discover how to easily create a Delta Lake table using the `datasets/ecomm_behavior_data/parquet/[sm|lg]/` data created in the [../notebooks/100-pre-processing/ecomm_csv_to_parquet.ipynb](./notebooks/100-pre-processing/ecomm_csv_to_parquet.ipynb).\n",
    "\n",
    "1. We will use the `parquet` data to convert to a [Delta Lake table](https://docs.delta.io/latest/delta-batch.html#create-a-table).\n",
    "2. We will also look at creating the table using the [DeltaTable](https://docs.delta.io/latest/api/python/index.html) builder methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fe3dc33-f63a-4551-baf6-fcb36aceedd6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/spark/work-dir/hitchhikers_guide/datasets/ecomm_behavior_data/parquet/lg/\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = '/opt/spark/work-dir/hitchhikers_guide/datasets/ecomm_behavior_data'\n",
    "# note: if you download the full dataset from https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store,\n",
    "# just use the following and comment out the `-sm.csv` datasets.\n",
    "datasets = ['2019-Oct.csv','2019-Nov.csv']\n",
    "#datasets = ['2019-Oct-sm.csv','2019-Nov-sm.csv']\n",
    "\n",
    "source_dir = 'sm' if datasets[1].endswith('-sm.csv') else 'lg'\n",
    "source_parquet = f\"{dataset_dir}/parquet/{source_dir}/\"\n",
    "\n",
    "# view the source parquet path\n",
    "print(source_parquet)\n",
    "\n",
    "# delta sink information\n",
    "delta_path = f\"{dataset_dir}/delta\"\n",
    "dl_table_name = \"ecomm\"\n",
    "\n",
    "# managed delta table\n",
    "dl_managed_name = \"ecomm_by_day\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25ff8d8-5ea1-42a4-96c9-a47b3cdd8cd3",
   "metadata": {},
   "source": [
    "# Delta Lake Tables\n",
    "We will learn to create an `empty` Delta Lake table next. There are many reasons that you'll want to create empty tables, for one, this allows you to create the `promise` of eventual data, while first getting things like the tables `schema` locked into place. If none of this is making sense yet, then never fear, you'll learn about `schemas` and `tblproperties` next.\n",
    "\n",
    "If you recall, we used a `StructType` to create a schema when we read the `ecomm_behavior_data` in the [100-pre-processing](../100-pre-processing/ecomm_csv_to_parquet.ipynb) notebook. The StructType is to DataFrames, like a structured data is to a Table row, both are strongly typed and provide a bit of peace of mind when working with a dataset. \n",
    "\n",
    "Structured Data is also one of the most important concepts to keep in mind while working with Streaming datasets.\n",
    "\n",
    "## Structured Data as our Data Contract\n",
    "Delta Lake uses a technique called `schema-on-write`. This means that all data being written by the `writer` or `producer` of a dataset must conform to a known `schema` after the initial `write` which in Delta Lake encapsulates a `transaction`. After the **initial write transaction**, which occurs at the time of table **creation**, a schema will exist. The importance of the `schema` is that it is `type-safe`. Type saftey with our data is also of critical importance for streaming, since a change in type, say from `string` to `integer` would break `backwards-compatibility` and `corrupt` our table. We don't want corrupt tables, so using `schema-on-write` and `schema-enforcement`, both tenents of the Delta Lake architecture, we can rest assured that any changes to our `schema` is backwards compatible*.\n",
    "\n",
    "> note and warning: (*) in the case where we must break backwards compatibility, we can, but it comes at the cost of `overwriting` the entire table and `schema`. This pattern is ripe for broken promises in the case where communication of a breaking-change, isn't broadcast to any downstream consumer (someone or some team that is relying on your data for their data product).\n",
    "\n",
    "We will create an Empty Table next that will hold our ecommerce data.\n",
    "\n",
    "## Creating an Unmanaged Delta Lake Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac87b7c1-f803-483a-83a9-e76eca20cb23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "struct<event_time:timestamp,event_type:string,product_id:int,category_id:bigint,category_code:string,brand:string,price:float,user_id:int,user_session:string,event_date:date>\n"
     ]
    }
   ],
   "source": [
    "# steal the schema pattern\n",
    "source_parquet = (spark.read\n",
    " .format(\"parquet\")\n",
    " .load(f\"{dataset_dir}/parquet/{source_dir}/\")\n",
    ")\n",
    "\n",
    "source_schema = source_parquet.schema.simpleString()\n",
    "# using the output of the schema from the reference `parquet` table, we can steal enough information to create our empty table\n",
    "print(source_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "831b346e-5f58-438a-9856-a89b0cff94f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Delta Lake Table Location on the File System\n",
    "# > note: Delta Lake tables come in two variants (unmanaged and managed)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS delta.`{delta_path}/{dl_table_name}` (\n",
    "        event_time TIMESTAMP,\n",
    "        event_type STRING,\n",
    "        product_id INTEGER,\n",
    "        category_id BIGINT,\n",
    "        category_code STRING,\n",
    "        brand STRING,\n",
    "        price FLOAT,\n",
    "        user_id INTEGER,\n",
    "        user_session STRING,\n",
    "        event_date DATE\n",
    "    ) USING DELTA\n",
    "    PARTITIONED BY (event_date)\n",
    "    TBLPROPERTIES('delta.logRetentionDuration'='interval 28 days');\n",
    "   \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd9c131-3750-455b-8a90-9d198e9f11cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "The prior `CREATE TABLE` command will generate an empty Delta Lake table. This doesn't mean that the `table` is actually empty though. The table contains `metadata` which provides information such as the table properties, partition columns, and table location information. Given the `source parquet data` is partitioned by `event_date`, we needed to preserve the `daily` partitions in our `parquet` table. This allows us to not think about how we partition as new data is being added to the table. Using the `event_date` table partitions will be written into without our supervision.\n",
    "\n",
    "If you have `tree` installed on your local machine, take a look at the output of calling:\n",
    "\n",
    "`tree ./hitchhikers_guide/datasets/ecomm_behavior_data/delta/`.\n",
    "\n",
    "```\n",
    "./hitchhikers_guide/datasets/ecomm_behavior_data/delta/\n",
    "└── ecomm\n",
    "    └── _delta_log\n",
    "        └── 00000000000000000000.json\n",
    "\n",
    "3 directories, 1 file\n",
    "```\n",
    "\n",
    "> Note: If you are using a mac and have brew installed. `brew install tree`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61898a62-0a0d-4f4a-b2e9-de3b93e05a36",
   "metadata": {},
   "source": [
    "## Populate our Empty Table using our Parquet Source Table\n",
    "In order to add records to our newly created `empty` table, we need to just read and write into the new table. \n",
    "\n",
    "> **NOTE**: If you are reading the entire october and november ecomm data you may see a JVM OOM followed by Py4JError: py4j does not exist in the JVM. This means the driver just crashed.\n",
    "\n",
    "> **TIP**: If you are importing the full 2 months of data. Use the isin to import more days at a time.\n",
    "> `.where(col(\"event_date\").isin(\"2019-10-01\",\"2019-10-02\",\"2019-10-03\"))`\n",
    "> This way you can also figure out at what point the data is too big and an inevitable crash will occur. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b57c94c-f1ed-4657-aa59-5ce8a31e90cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# providing the source_parquet for completeness. This has been created earlier in the notebook to steal the parquet schema.\n",
    "\n",
    "# if you want to play around with resolving OOM on the big dataset, don't import by day, and watch things fall over. (~14gb into 1gb memory)...\n",
    "#source_parquet = (spark.read\n",
    "# .format(\"parquet\")\n",
    "# .load(f\"{dataset_dir}/parquet/{source_dir}/\")\n",
    "#)\n",
    "\n",
    "source_parquet = (spark.read\n",
    "  .format(\"parquet\")\n",
    "  .load(f\"{dataset_dir}/parquet/{source_dir}/\")\n",
    "  #.where(col(\"event_date\").eqNullSafe(\"2019-10-01\"))\n",
    "  .where(col(\"event_date\").isin(\"2019-11-29\",\"2019-11-30\"))\n",
    ")\n",
    "\n",
    "# TIP: sometimes you just want to make sure the data exists\n",
    "# if you view what you are going to write, then you can see if the upstream is empty\n",
    "# with a quick visual (when you are in notebooks), when you are running outside of notebooks\n",
    "# you'll need to rely on `count()`, or other file listing techniques to see if the data\n",
    "# exists, otherwise, your import job could pass with flying colors - while there is still sadly\n",
    "# no data being moved from a -> b.\n",
    "\n",
    "#source_parquet.show(10)\n",
    "\n",
    "(source_parquet\n",
    " .write\n",
    " .format(\"delta\")\n",
    " .option(\"path\", f\"{delta_path}/{dl_table_name}\")\n",
    " .mode(\"append\")\n",
    " .save()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3ab7b8-e248-46d7-a878-e1b4fb859f51",
   "metadata": {},
   "source": [
    "1. What have you learned in the process of reading the source parquet into the new Delta Table location?\n",
    "2. What patterns have you picked up here? Did you try playing with different strategies for selecting data using the `.where(col(\"event_date\")....)`? What about using a collection of dates, or matches like `2019-10-1*`? If you are new to using PySpark or Spark in general, take a look at the [pyspark.sql.functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html) package to help you on your way.\n",
    "3. If you were using the large datasets, what kinds of issues did you run into? \n",
    "\n",
    "Depending on how many actions you were taking. You probably saw: \n",
    "\n",
    "```\n",
    "[warning][gc,alloc] Executor task launch worker for task 5.0 in stage 603.0 (TID 8573): Retried waiting for GCLocker too often allocating 262144 words\n",
    "23/06/19 21:22:03 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again\n",
    "```\n",
    "\n",
    "Learning to use Warnings and Exceptions to your advantage can be really helpful to understand what sorts of pressure points exist in your applications. It is also much more fun to break things locally, when we aren't experiencing problems in production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428900e8-ef29-408b-94d7-fbfc6bad63a7",
   "metadata": {},
   "source": [
    "### What Makes up a Delta Lake Table?\n",
    "The Delta Lake table is comprised of the `_delta_log` directory, as well as optional `partition` based directories, or in the case of simple tables, just a collection of `part-{uuid}.c000.{compression}.parquet`, which would populate the `partition based directories` as well.\n",
    "\n",
    "```\n",
    "./hitchhikers_guide/datasets/ecomm_behavior_data/delta/\n",
    "└── ecomm\n",
    "    ├── _delta_log\n",
    "    │   ├── 00000000000000000000.json\n",
    "    │   ├── 00000000000000000001.json\n",
    "    └── event_date=2019-10-01\n",
    "        ├── part-00002-abb10ec6-6425-4ef1-91e8-ccb05489fa35.c000.zstd.parquet\n",
    "        ├── part-00004-2eda7d21-dcf8-48b5-8e76-0dc6e71575d2.c000.zstd.parquet\n",
    "```\n",
    "\n",
    "You will notice the `zstd` compression. ZSTD compression is like compression on sterioids. We set this earlier on in the notebook using `spark.conf.set(\"spark.sql.parquet.compression.codec\", \"zstd\")`. \n",
    "\n",
    "For comparison, if you comment out the line in the first cell of the notebook, Spark will use the default `snappy` compression codec. This is still a powerful compression codec, but for a size-on-disk comparision.\n",
    "\n",
    "```\n",
    "ls -lh ./hitchhikers_guide/datasets/ecomm_behavior_data/delta/ecomm/event_date=2019-10-01\n",
    "-rw-r--r--  1 {me}  staff    37M Jun 19 13:57 part-00002-3e39ad07-cc35-4ef1-a0eb-77652c3cbc07.c000.snappy.parquet\n",
    "-rw-r--r--  1 {me}  staff   5.6M Jun 19 13:57 part-00004-fb1843f4-bdb7-4578-b14a-257b2525f6b5.c000.snappy.parquet\n",
    "-rw-r--r--  1 {me}  staff    18M Jun 19 13:59 part-00002-abb10ec6-6425-4ef1-91e8-ccb05489fa35.c000.zstd.parquet\n",
    "-rw-r--r--  1 {me}  staff   3.7M Jun 19 13:59 part-00004-2eda7d21-dcf8-48b5-8e76-0dc6e71575d2.c000.zstd.parquet\n",
    "```\n",
    "\n",
    "> For the exact same data, the zstd compression results in a ~48% reduction in size from (37mb->18mb) and a ~66% reduction in size from 5.6mb->3.7mb. That is over 50% size reduction which is bonkers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7b66cb-d6f8-4990-8a67-62570104810b",
   "metadata": {},
   "source": [
    "## Converting an existing External Delta Lake Table to a Managed Table\n",
    "The Delta Lake table we created uses the `./delta/ecomm/` path on the filesystem. This means we need to understand where in the world a given Table lives, which is not a big problem when there are only a few tables (probably stored somewhere using AWS S3 or Azure Blob Storage, or Google Cloud Storage), but this becomes more problematic as more and more tables become available. At a certain point, it becomes essential to use Managed tables.\n",
    "\n",
    "> Managed Delta Lake tables use the Hive Metastore (or hive compatible metastore) for OSS Delta, and if you're working inside Databricks, you can just use [Unity Catalog](https://www.databricks.com/product/unity-catalog) to mix access, authentication alongside your Table metadata.\n",
    "\n",
    "Given this project is all about using OSS Delta, we're riding the `local` spark-warehouse route, which can be seen under the `spark-warehouse` directory to the left of this notebook (in the filesystem view). You'll also notice there is a `metastore_db`. This directory stores the information commonly stored in the Hive Metastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1315b44-efc1-44a3-bd99-b6ed5d89b2cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if you want to check what current databases exist, or what tables exist you can use the following.\n",
    "spark.catalog.listDatabases()\n",
    "spark.catalog.listTables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b654e8-569e-4b55-a9f5-d0cfaf82e31f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the Managed Table Definition\n",
    "# also note - we are keeping the `unmanaged` location in tact and copying files into the new location\n",
    "# The only difference between the external table definition and the managed table definition is the `database.table` vs the `delta.\n",
    "spark.sql(f\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS default.`{dl_managed_name}` (\n",
    "    event_time TIMESTAMP,\n",
    "    event_type STRING,\n",
    "    product_id INTEGER,\n",
    "    category_id BIGINT,\n",
    "    category_code STRING,\n",
    "    brand STRING,\n",
    "    price FLOAT,\n",
    "    user_id INTEGER,\n",
    "    user_session STRING,\n",
    "    event_date DATE\n",
    "  ) USING DELTA\n",
    "  PARTITIONED BY (event_date)\n",
    "  TBLPROPERTIES(\n",
    "    'delta.logRetentionDuration'='interval 28 days',\n",
    "    'catalog.team_name'='dldg_authors',\n",
    "    'catalog.engineering.comms.slack'='https://delta-users.slack.com/archives/CG9LR6LN4'\n",
    "  );\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8fb30a-9c28-4dcc-ae50-bf38d162f7b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add some additional table properties like the table classification (pii? all-access?)\n",
    "spark.sql(f\"\"\"\n",
    "  ALTER TABLE default.`{dl_managed_name}` \n",
    "  SET TBLPROPERTIES (\n",
    "    'catalog.engineering.comms.email'='dldg_authors@gmail.com',\n",
    "    'catalog.table.classification'='all-access'\n",
    "  )\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469a4fb9-1922-4796-9eeb-90b9e0810a09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a little glimpse into the table history (what actions have occured - otherwise known as transactions)\n",
    "(DeltaTable.forName(spark, f\"default.{dl_managed_name}\")\n",
    " .history(10)\n",
    " .select(\"version\", \"timestamp\", \"operation\", \"operationParameters\")\n",
    " .show(10, truncate=True, vertical=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a84ed6c2-610f-4b74-acb5-12ce2bbca79b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<delta.tables.DeltaTable at 0xffff7688ec40>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the createIfNotExists utility\n",
    "# can be used instead of the prior two cells\n",
    "(DeltaTable.createIfNotExists(spark)\n",
    "    .tableName(f\"default.{dl_managed_name}\")\n",
    "    .property(\"description\", \"Retail Ecomm Dataset. This can be used to forecast holiday seasonality for multiple categories\")\n",
    "    .addColumn(\"event_time\", \"TIMESTAMP\")\n",
    "    .addColumn(\"event_type\", \"STRING\")\n",
    "    .addColumn(\"product_id\", \"INTEGER\")\n",
    "    .addColumn(\"category_id\", \"BIGINT\")\n",
    "    .addColumn(\"brand\", \"STRING\")\n",
    "    .addColumn(\"price\", \"FLOAT\")\n",
    "    .addColumn(\"user_id\", \"INTEGER\")\n",
    "    .addColumn(\"user_session\", \"STRING\")\n",
    "    .addColumn(\"event_date\", \"DATE\")\n",
    "    .partitionedBy(\"event_date\")\n",
    "    .property(\"catalog.team_name\", \"dldg_authors\")\n",
    "    .property(\"catalog.engineering.comms.slack\",\n",
    "\t\"https://delta-users.slack.com/archives/CG9LR6LN4\")\n",
    "    .property(\"catalog.engineering.comms.email\",\"dldg_authors@gmail.com\")\n",
    "    .property(\"catalog.table.classification\",\"all-access\")\n",
    "    .execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7610ae2-446e-44e8-9432-c2f1af71215a",
   "metadata": {},
   "source": [
    "The only immediate difference between creating a non-managed table and a managed table all comes down to the table location: \n",
    "\n",
    "```\n",
    "delta.`{delta_path}/{dl_table_name}` vs default.`{dl_managed_name}`\n",
    "```\n",
    "\n",
    "With the managed table, you can also create additional `databases` using the `CREATE DATABASE` syntax. We are currenlty using the `default` database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87401fa5-ed74-43cf-b2ff-f81a47f1d832",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/spark/work-dir/hitchhikers_guide/datasets/ecomm_behavior_data/delta/ecomm\n"
     ]
    }
   ],
   "source": [
    "# see the unmanaged Delta Table path\n",
    "print(f\"{delta_path}/{dl_table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "484f8ab1-b7b9-4d4b-801c-676f4bd8b7bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# look up the unmanaged source table by path\n",
    "#dt = DeltaTable.forPath(spark, f\"{delta_path}/{dl_table_name}\").detail().show(1, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b48b9f80-40da-4c33-bf4e-389f7c863657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:/opt/spark/work-dir/hitchhikers_guide/warehouse'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note: the startup.sh adds this information to the session. Having a shared warehouse directory allows us to reuse tables between notebooks\n",
    "spark.conf.get(\"spark.sql.warehouse.dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92651bec-01bb-4cc2-b8fb-52e3de5b0664",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read from the external Delta Lake location\n",
    "# write into the Managed Delta Lake location\n",
    "\n",
    "sourceTable = DeltaTable.forPath(spark, f\"{delta_path}/{dl_table_name}\")\n",
    "tableDf = sourceTable.toDF()\n",
    "(\n",
    "    tableDf\n",
    "    .where(col(\"event_date\").eqNullSafe(\"2019-10-01\"))\n",
    "    #.where(col(\"event_date\").isin(\"2019-11-28\", \"2019-11-29\", \"2019-11-30\"))\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .saveAsTable(f\"default.{dl_managed_name}\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa66f71-f36c-4661-a73e-5a5f55418f29",
   "metadata": {},
   "source": [
    "## Inspect the Table\n",
    "> Now that you have data in your unmanaged table. Take a peek. What is interesting to you?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65543fcf-c3e5-413b-937e-cc924db5acb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " format           | delta                                                                                                                                                                                                                                                                           \n",
      " id               | 7e870b36-805a-498b-979f-3760e8badd29                                                                                                                                                                                                                                            \n",
      " name             | spark_catalog.default.ecomm_by_day                                                                                                                                                                                                                                              \n",
      " description      | NULL                                                                                                                                                                                                                                                                            \n",
      " location         | file:/opt/spark/work-dir/hitchhikers_guide/warehouse/ecomm_by_day                                                                                                                                                                                                               \n",
      " createdAt        | 2023-10-23 19:25:41.152                                                                                                                                                                                                                                                         \n",
      " lastModified     | 2023-10-23 19:30:54.458                                                                                                                                                                                                                                                         \n",
      " partitionColumns | [event_date]                                                                                                                                                                                                                                                                    \n",
      " numFiles         | 149                                                                                                                                                                                                                                                                             \n",
      " sizeInBytes      | 2079549053                                                                                                                                                                                                                                                                      \n",
      " properties       | {catalog.table.classification -> all-access, catalog.engineering.comms.email -> dldg_authors@gmail.com, delta.logRetentionDuration -> interval 28 days, catalog.team_name -> dldg_authors, catalog.engineering.comms.slack -> https://delta-users.slack.com/archives/CG9LR6LN4} \n",
      " minReaderVersion | 1                                                                                                                                                                                                                                                                               \n",
      " minWriterVersion | 2                                                                                                                                                                                                                                                                               \n",
      " tableFeatures    | [appendOnly, invariants]                                                                                                                                                                                                                                                        \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# peek at table details\n",
    "(DeltaTable.forName(spark, f\"default.{dl_managed_name}\")\n",
    "   .detail()\n",
    "   .show(1, truncate=False, vertical=True)\n",
    ")\n",
    "# view differences in describe extended\n",
    "#spark.sql(\"describe extended default.ecomm_by_day\").show(truncate=True, vertical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca47747f-84bc-46ce-ab23-689ed8aa57e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'catalog.table.classification': 'all-access',\n",
       " 'delta.logRetentionDuration': 'interval 28 days',\n",
       " 'catalog.team_name': 'dldg_authors',\n",
       " 'catalog.engineering.comms.slack': 'https://delta-users.slack.com/archives/CG9LR6LN4',\n",
       " 'catalog.engineering.comms.email': 'dldg_authors@gmail.com'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at just the Table Properties\n",
    "table_info = DeltaTable.forName(spark, \"default.ecomm_by_day\").detail()\n",
    "# view the table properties locally (call first then slice by the properties index)\n",
    "tblproperties = table_info.first()['properties']\n",
    "tblproperties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b068932e-9b99-4f85-833c-b864bc0de2cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " event_time    | 2019-10-21 19:47:52  \n",
      " event_type    | view                 \n",
      " product_id    | 4804056              \n",
      " category_id   | 2053013554658804075  \n",
      " category_code | electronics.audio... \n",
      " brand         | apple                \n",
      " price         | 160.34               \n",
      " user_id       | 513066676            \n",
      " user_session  | cafca09f-7785-461... \n",
      " event_date    | 2019-10-21           \n",
      "-RECORD 1-----------------------------\n",
      " event_time    | 2019-10-21 19:47:52  \n",
      " event_type    | view                 \n",
      " product_id    | 12300181             \n",
      " category_id   | 2053013556311359947  \n",
      " category_code | construction.tool... \n",
      " brand         | dewalt               \n",
      " price         | 356.2                \n",
      " user_id       | 553528018            \n",
      " user_session  | 002f83d7-f527-47e... \n",
      " event_date    | 2019-10-21           \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(f\"default.{dl_managed_name}\").show(2, truncate=True, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910ef125-8021-4a75-a968-bbb22a06fc3d",
   "metadata": {},
   "source": [
    "## Converting an existing Parquet Table to Delta Lake\n",
    "Using the `convertToDelta` method via the `DeltaTable` python utility enables us to easily create our Delta Lake table in place. In place just means that the table will not have to be copied and moved, furthermore, since Delta Lake uses Parquet all that is modified is the addition of the `_delta_log` file in the root of the table. \n",
    "\n",
    "> note: if you want to use the convertToDelta utility function, just uncomment the following cell and run it, otherwise, skip on to creating a new Delta Lake table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e419977-aa92-4fd2-af5d-7fcbb326b4d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The convertToDelta method will take an existing `Parquet` table, and convert it in place to a DeltaLake table.\n",
    "#parquet_table_dir = f\"{dataset_dir}/parquet/{source_dir}/\"\n",
    "# dt = DeltaTable.convertToDelta(spark, f\"parquet.`{parquet_table_dir}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e27049-8af7-40d6-a062-493b0e7e8d08",
   "metadata": {
    "tags": []
   },
   "source": [
    "# What We Learned\n",
    "1. How to use an existing Parquet Table to create an Unmanaged and Managed Delta Lake Table\n",
    "2. How to View the Table Metadata using the `describe extended table` SQL command and the `DeltaTable.forName...detail()` view.\n",
    "3. How to slice the Table detail and view the `properties`. This allows us to quickly view important metadata about the Delta Lake table and we'll see how to use the Table Properties for more and more in other parts of the Guide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cd4cf6-f535-45b7-b14e-1e410aa8ac87",
   "metadata": {
    "tags": []
   },
   "source": [
    "## What's Next?\n",
    "Now we can use the Managed `ecomm` table as we explore how to use Delta Lake Streaming. \n",
    "\n",
    "1. [Delta Lake Streaming 101](./dl-streaming-101.ipynb) is a gentle introduction to Delta Lake Streaming. This is a necessary part of learning how to effectively use Delta Lake for fun and profit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
