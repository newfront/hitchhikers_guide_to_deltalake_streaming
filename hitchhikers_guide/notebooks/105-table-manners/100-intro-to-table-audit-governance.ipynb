{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79e31d88-4324-4b52-8d81-3b80cbf64e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "table_path = \"/opt/spark/work-dir/hitchhikers_guide/datasets/ecomm_behavior_data/delta/ecomm/\"\n",
    "\n",
    "# using an existing table (we must not screw this up!)\n",
    "dt = DeltaTable.forPath(spark, table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d05f62-955d-41ba-a8bf-29879b1032ff",
   "metadata": {},
   "source": [
    "# Governance 101\n",
    "> We will walk through a simple method to automatically retain Delta tables for N-days\n",
    "> It is also worth pointing out that in the example, the default state is \"off\". This is a saftey precausion given that not all \"tables\" will have the same retention interval, and it is best to ensure that \"we can\" test drive before flipping any of these table based feature-flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d21d89e0-f505-4b41-a82c-ded94f0f524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, col, make_interval, make_dt_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0601cbb1-a7d1-49e4-9503-dd9144f0056b",
   "metadata": {},
   "source": [
    "> Note: Given that there is a local hive-metastore running, the first time this runs might take a bit to on-demand get all the jars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27456fe-9f0c-4eb5-b485-2bd15ca28cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE delta.`{table_path}`\n",
    "SET TBLPROPERTIES (\n",
    "  'catalog.table.gov.retention.enabled'='true',\n",
    "  'catalog.table.gov.retention.date_col'='event_date',\n",
    "  'catalog.table.gov.retention.policy'='interval 28 days'\n",
    ")\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1952bb-4f48-4c99-9f5a-21cd2e659e69",
   "metadata": {},
   "source": [
    "## How to Fetch the Governance Properties\n",
    "1. We need to understand if \"governance\" is enabled on the table\n",
    "2. If it is enabled, then we need to fetch the retention policy, or default to a sane retention (90 days in the example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac6b7a1-8ba6-4f5c-9ad0-6f6a515461d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "props = dt.detail().first()['properties']\n",
    "table_retention_enabled = bool(props.get('catalog.table.gov.retention.enabled', 'false'))\n",
    "table_retention_policy = props.get('catalog.table.gov.retention.policy', 'interval 90 days')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95981858-2140-45b6-8a92-b39a843dc037",
   "metadata": {},
   "source": [
    "## Generate a Function to parse the `interval` string\n",
    "> The following `convert_to_interval` method is simple for the example. This could easily be a PySpark UDF that takes a Column of `StringType` to extract the interval and convert to the catalyst `IntervalType`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a244f964-e591-4096-b0fa-709ef30a7ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql.functions import lit, col, make_interval, make_dt_interval\n",
    "def convert_to_interval(interval: str):\n",
    "    \"\"\"\n",
    "    Supports extraction to make_dt_interval([days, hours, mins, secs])\n",
    "    note: secs is a decimal, this function only uses secs for simplicity\n",
    "    \"\"\"\n",
    "    target = str.lower(interval).lstrip()\n",
    "    target = target.replace(\"interval\", \"\").lstrip() if target.startswith(\"interval\") else target\n",
    "    number, interval_type = re.split(\"\\s+\", target)\n",
    "    amount = int(number)\n",
    "    dt_interval = [None, None, None, None]\n",
    "    if interval_type == \"days\":\n",
    "        dt_interval[0] = lit(364 if amount > 365 else amount)\n",
    "    elif interval_type == \"hours\":\n",
    "        dt_interval[1] = lit(23 if amount > 24 else amount)\n",
    "    elif interval_type == \"mins\":\n",
    "        dt_interval[2] = lit(59 if amount > 60 else amount)\n",
    "    elif interval_type == \"secs\":\n",
    "        dt_interval[3] = lit(59 if amount > 60 else amount)\n",
    "    else:\n",
    "        raise RuntimeException(f\"Unknown interval_type {interval_type}\")\n",
    "    \n",
    "    return make_dt_interval(\n",
    "        days=dt_interval[0],\n",
    "        hours=dt_interval[1],\n",
    "        mins=dt_interval[2],\n",
    "        secs=dt_interval[3]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53c30681-4d95-46d7-a0aa-d4c0957954fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+------------------------------------+------------+\n",
      "|now                       |retention_interval                  |retain_after|\n",
      "+--------------------------+------------------------------------+------------+\n",
      "|2024-06-03 19:19:33.554395|INTERVAL '28 00:00:00' DAY TO SECOND|2024-05-06  |\n",
      "+--------------------------+------------------------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "interval = convert_to_interval(table_retention_policy)\n",
    "\n",
    "rules = (\n",
    "    spark.sql(\"select current_timestamp() as now\")\n",
    "    .withColumn(\"retention_interval\", interval)\n",
    "    .withColumn(\"retain_after\", to_date((col(\"now\")-col(\"retention_interval\"))))\n",
    ")\n",
    "\n",
    "rules.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b039861-58f3-406c-902b-974ee3649052",
   "metadata": {},
   "source": [
    "### Where to Go Next\n",
    "Now that we've created the rules engine to take reusable \"intervals\" from our Delta tables, we can use `conditional` delete's to ensure we only retain the correct amount of data in our tables.\n",
    "\n",
    "> Note: This pattern works for timeseries tables. In the case where there is \"timeless\" data, this pattern won't do much good :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfde9be-b8a7-4477-9b45-724e574cb8cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
